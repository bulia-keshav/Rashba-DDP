{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Descriptor Extraction\n",
    "Extracts orbital-weight-based descriptors from `vasprun.xml` for all Rashba compounds.\n",
    "\n",
    "**Descriptor Types:**\n",
    "- **A** `WM_total`: sum(w_X * M_X) at VBM/CBM\n",
    "- **B** `WM_p_only`: sum(p_X * M_X) using only p-orbital contributions\n",
    "- **C** `WM_p_frac`: p_i*M_i / sum(p_j*M_j) for top 2 heaviest elements\n",
    "- **D** `WM_indiv`: w_heavy1 * M_heavy1, w_heavy2 * M_heavy2 individually\n",
    "- **E** `p_frac`: total p-orbital fraction\n",
    "- **F** `p_heavy`: p_frac_of_heaviest * M_heaviest for top 2\n",
    "\n",
    "**Windows:** 0.05, 0.1, 0.5 eV\n",
    "\n",
    "**Target:** max(Rashba_parameter) per UID from rashba.csv\n",
    "\n",
    "**Output CSVs:** one per (type, window) combo + combined CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pymatgen.io.vasp.outputs import Vasprun\n",
    "from pymatgen.electronic_structure.core import Spin, OrbitalType\n",
    "from pymatgen.core.periodic_table import Element\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATHS - ADJUST THESE\n",
    "# ============================================================\n",
    "BASE_DIR = r\"C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\"\n",
    "RASHBA_CSV = os.path.join(BASE_DIR, \"Data\", \"rashba.csv\")\n",
    "RASHBA_DIR = os.path.join(BASE_DIR, \"Inverse-design\", \"rashba\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"Weight-contribution\", \"contribution-model\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Windows\n",
    "WINDOWS = [0.05, 0.1, 0.5, 1.0]\n",
    "\n",
    "print(f\"Rashba CSV: {RASHBA_CSV}\")\n",
    "print(f\"Rashba compounds dir: {RASHBA_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD RASHBA CSV & GET TARGET (max alpha_R per UID)\n",
    "# ============================================================\n",
    "df_rashba = pd.read_csv(RASHBA_CSV)\n",
    "print(f\"Total rows in rashba.csv: {len(df_rashba)}\")\n",
    "print(f\"Unique UIDs: {df_rashba['uid'].nunique()}\")\n",
    "print(f\"Columns: {list(df_rashba.columns)}\")\n",
    "\n",
    "# Max Rashba parameter per UID\n",
    "target = df_rashba.groupby('uid')['Rashba_parameter'].max().reset_index()\n",
    "target.columns = ['uid', 'alpha_R_max']\n",
    "\n",
    "# Also keep formula for reference\n",
    "uid_formula = df_rashba.groupby('uid')['Formula'].first().reset_index()\n",
    "target = target.merge(uid_formula, on='uid')\n",
    "\n",
    "print(f\"\\nTarget: {len(target)} compounds with max alpha_R\")\n",
    "print(target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIND VASPRUN FILES FOR EACH UID\n",
    "# ============================================================\n",
    "# Folder structure: rashba/AsBrTe-671e6de2497a/ss_2d%2FAsBrTe-671e6de2497a%2Fbands_ncl%2Fvasprun.xml\n",
    "# UID in CSV: 671e6de2497a (last part after dash)\n",
    "\n",
    "compound_dirs = {}\n",
    "for folder in os.listdir(RASHBA_DIR):\n",
    "    folder_path = os.path.join(RASHBA_DIR, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Build expected vasprun path: ss_2d%2F{folder}%2Fbands_ncl%2Fvasprun.xml\n",
    "    vasprun_name = f\"ss_2d%2F{folder}%2Fbands_ncl%2Fvasprun.xml\"\n",
    "    vasprun_path = os.path.join(folder_path, vasprun_name)\n",
    "    \n",
    "    # Also try glob as fallback\n",
    "    if not os.path.exists(vasprun_path):\n",
    "        candidates = glob.glob(os.path.join(folder_path, \"*vasprun*\"))\n",
    "        if candidates:\n",
    "            vasprun_path = candidates[0]\n",
    "        else:\n",
    "            # Try recursive\n",
    "            candidates = glob.glob(os.path.join(folder_path, \"**\", \"vasprun.xml\"), recursive=True)\n",
    "            if candidates:\n",
    "                vasprun_path = candidates[0]\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # Extract UID: everything after the LAST dash\n",
    "    # e.g., AsBrTe-671e6de2497a -> 671e6de2497a\n",
    "    # e.g., Bi2P2S6-287dcf4f1a19 -> 287dcf4f1a19\n",
    "    last_dash = folder.rfind('-')\n",
    "    if last_dash >= 0:\n",
    "        uid_from_folder = folder[last_dash+1:]\n",
    "    else:\n",
    "        uid_from_folder = folder\n",
    "    \n",
    "    compound_dirs[uid_from_folder] = {\n",
    "        'folder': folder,\n",
    "        'vasprun': vasprun_path\n",
    "    }\n",
    "\n",
    "print(f\"Found {len(compound_dirs)} compound directories with vasprun.xml\")\n",
    "\n",
    "# Match with target UIDs from CSV\n",
    "matched = 0\n",
    "unmatched_uids = []\n",
    "for _, row in target.iterrows():\n",
    "    uid = row['uid']\n",
    "    # Try direct match\n",
    "    if uid in compound_dirs:\n",
    "        matched += 1\n",
    "        continue\n",
    "    # Try: CSV uid might be longer/shorter than folder uid\n",
    "    # e.g., CSV has 'da5fd2bb4' but folder has 'da5fd2bb4xxx' or vice versa\n",
    "    found = False\n",
    "    for folder_uid in list(compound_dirs.keys()):\n",
    "        if uid.startswith(folder_uid) or folder_uid.startswith(uid):\n",
    "            compound_dirs[uid] = compound_dirs[folder_uid]\n",
    "            matched += 1\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        unmatched_uids.append((uid, row['Formula']))\n",
    "\n",
    "print(f\"Matched: {matched}/{len(target)}\")\n",
    "if unmatched_uids:\n",
    "    print(f\"\\nUnmatched ({len(unmatched_uids)}):\")\n",
    "    for uid, formula in unmatched_uids[:15]:\n",
    "        print(f\"  {formula} - {uid}\")\n",
    "\n",
    "# Show a few matches for sanity check\n",
    "print(f\"\\nSample matches:\")\n",
    "for i, (uid, info) in enumerate(list(compound_dirs.items())[:3]):\n",
    "    print(f\"  UID: {uid} -> {info['folder']}\")\n",
    "    print(f\"    vasprun: {os.path.basename(info['vasprun'])}\")\n",
    "    print(f\"    exists: {os.path.exists(info['vasprun'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORE FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def get_dos_array(dos_obj):\n",
    "    \"\"\"Extract DOS values, summing spin channels for SOC.\"\"\"\n",
    "    if Spin.down in dos_obj.densities:\n",
    "        return dos_obj.densities[Spin.up] + dos_obj.densities[Spin.down]\n",
    "    return dos_obj.densities[Spin.up]\n",
    "\n",
    "def integrate_window(energies, dos_vals, window):\n",
    "    \"\"\"Integrate DOS in energy window using trapezoidal rule.\"\"\"\n",
    "    mask = (energies >= window[0]) & (energies <= window[1])\n",
    "    e_w = energies[mask]\n",
    "    d_w = dos_vals[mask]\n",
    "    if len(e_w) < 2:\n",
    "        return 0.0\n",
    "    return np.trapezoid(d_w, e_w)\n",
    "\n",
    "def extract_contributions(vasprun_path, window_size):\n",
    "    \"\"\"\n",
    "    Parse vasprun.xml and extract orbital/atomic contributions at VBM/CBM.\n",
    "    Returns dict with per-element per-orbital contributions (normalized).\n",
    "    \"\"\"\n",
    "    vr = Vasprun(vasprun_path, parse_dos=True, parse_eigen=False)\n",
    "    cdos = vr.complete_dos\n",
    "    e_fermi = vr.efermi\n",
    "    \n",
    "    vbm = cdos.get_cbm_vbm()[1]\n",
    "    cbm = cdos.get_cbm_vbm()[0]\n",
    "    \n",
    "    vbm_s = vbm - e_fermi\n",
    "    cbm_s = cbm - e_fermi\n",
    "    energies = cdos.energies - e_fermi\n",
    "    \n",
    "    vbm_win = [vbm_s - window_size, vbm_s]\n",
    "    cbm_win = [cbm_s, cbm_s + window_size]\n",
    "    \n",
    "    element_dos = cdos.get_element_dos()\n",
    "    orbital_map = {'s': OrbitalType.s, 'p': OrbitalType.p, 'd': OrbitalType.d}\n",
    "    \n",
    "    # Raw contributions\n",
    "    raw = {}\n",
    "    for element in element_dos:\n",
    "        el = str(element)\n",
    "        mass = Element(el).atomic_mass\n",
    "        Z = Element(el).Z\n",
    "        raw[el] = {'mass': float(mass), 'Z': Z}\n",
    "        \n",
    "        spd = cdos.get_element_spd_dos(element)\n",
    "        for orb_str in ['s', 'p', 'd']:\n",
    "            orb_type = orbital_map[orb_str]\n",
    "            if orb_type in spd:\n",
    "                dos_vals = get_dos_array(spd[orb_type])\n",
    "                raw[el][f'{orb_str}_VBM'] = integrate_window(energies, dos_vals, vbm_win)\n",
    "                raw[el][f'{orb_str}_CBM'] = integrate_window(energies, dos_vals, cbm_win)\n",
    "            else:\n",
    "                raw[el][f'{orb_str}_VBM'] = 0.0\n",
    "                raw[el][f'{orb_str}_CBM'] = 0.0\n",
    "    \n",
    "    # Total per band edge\n",
    "    total_vbm = sum(raw[el][f'{o}_VBM'] for el in raw for o in ['s','p','d'])\n",
    "    total_cbm = sum(raw[el][f'{o}_CBM'] for el in raw for o in ['s','p','d'])\n",
    "    \n",
    "    # Normalized\n",
    "    for el in raw:\n",
    "        for o in ['s','p','d']:\n",
    "            raw[el][f'{o}_VBM_norm'] = raw[el][f'{o}_VBM'] / total_vbm if total_vbm > 0 else 0\n",
    "            raw[el][f'{o}_CBM_norm'] = raw[el][f'{o}_CBM'] / total_cbm if total_cbm > 0 else 0\n",
    "        # Element-level (sum of orbitals)\n",
    "        raw[el]['w_VBM'] = sum(raw[el][f'{o}_VBM_norm'] for o in ['s','p','d'])\n",
    "        raw[el]['w_CBM'] = sum(raw[el][f'{o}_CBM_norm'] for o in ['s','p','d'])\n",
    "        # p-orbital fraction for this element\n",
    "        raw[el]['p_VBM'] = raw[el]['p_VBM_norm']\n",
    "        raw[el]['p_CBM'] = raw[el]['p_CBM_norm']\n",
    "    \n",
    "    return raw\n",
    "\n",
    "def compute_descriptors(raw):\n",
    "    \"\"\"\n",
    "    From raw contributions dict, compute all descriptor types.\n",
    "    Returns flat dict of descriptors.\n",
    "    \"\"\"\n",
    "    desc = {}\n",
    "    elements = list(raw.keys())\n",
    "    \n",
    "    # Sort by mass (heaviest first)\n",
    "    sorted_els = sorted(elements, key=lambda x: raw[x]['mass'], reverse=True)\n",
    "    heavy1 = sorted_els[0] if len(sorted_els) >= 1 else None\n",
    "    heavy2 = sorted_els[1] if len(sorted_els) >= 2 else None\n",
    "    \n",
    "    # --- Type A: WM_total = sum(w_X * M_X) ---\n",
    "    desc['A_WM_VBM'] = sum(raw[el]['w_VBM'] * raw[el]['mass'] for el in elements)\n",
    "    desc['A_WM_CBM'] = sum(raw[el]['w_CBM'] * raw[el]['mass'] for el in elements)\n",
    "    \n",
    "    # --- Type B: WM_p_only = sum(p_X * M_X) ---\n",
    "    desc['B_WMp_VBM'] = sum(raw[el]['p_VBM'] * raw[el]['mass'] for el in elements)\n",
    "    desc['B_WMp_CBM'] = sum(raw[el]['p_CBM'] * raw[el]['mass'] for el in elements)\n",
    "    \n",
    "    # --- Type C: WM_p_frac = p_i*M_i / sum(p_j*M_j) for top 2 heaviest ---\n",
    "    denom_vbm = sum(raw[el]['p_VBM'] * raw[el]['mass'] for el in elements)\n",
    "    denom_cbm = sum(raw[el]['p_CBM'] * raw[el]['mass'] for el in elements)\n",
    "    \n",
    "    if heavy1:\n",
    "        desc['C_pfrac_h1_VBM'] = (raw[heavy1]['p_VBM'] * raw[heavy1]['mass']) / denom_vbm if denom_vbm > 0 else 0\n",
    "        desc['C_pfrac_h1_CBM'] = (raw[heavy1]['p_CBM'] * raw[heavy1]['mass']) / denom_cbm if denom_cbm > 0 else 0\n",
    "    else:\n",
    "        desc['C_pfrac_h1_VBM'] = 0\n",
    "        desc['C_pfrac_h1_CBM'] = 0\n",
    "    \n",
    "    if heavy2:\n",
    "        desc['C_pfrac_h2_VBM'] = (raw[heavy2]['p_VBM'] * raw[heavy2]['mass']) / denom_vbm if denom_vbm > 0 else 0\n",
    "        desc['C_pfrac_h2_CBM'] = (raw[heavy2]['p_CBM'] * raw[heavy2]['mass']) / denom_cbm if denom_cbm > 0 else 0\n",
    "    else:\n",
    "        desc['C_pfrac_h2_VBM'] = 0\n",
    "        desc['C_pfrac_h2_CBM'] = 0\n",
    "    \n",
    "    # --- Type D: WM_indiv = w_heavyN * M_heavyN ---\n",
    "    if heavy1:\n",
    "        desc['D_wm_h1_VBM'] = raw[heavy1]['w_VBM'] * raw[heavy1]['mass']\n",
    "        desc['D_wm_h1_CBM'] = raw[heavy1]['w_CBM'] * raw[heavy1]['mass']\n",
    "    else:\n",
    "        desc['D_wm_h1_VBM'] = 0\n",
    "        desc['D_wm_h1_CBM'] = 0\n",
    "    \n",
    "    if heavy2:\n",
    "        desc['D_wm_h2_VBM'] = raw[heavy2]['w_VBM'] * raw[heavy2]['mass']\n",
    "        desc['D_wm_h2_CBM'] = raw[heavy2]['w_CBM'] * raw[heavy2]['mass']\n",
    "    else:\n",
    "        desc['D_wm_h2_VBM'] = 0\n",
    "        desc['D_wm_h2_CBM'] = 0\n",
    "    \n",
    "    # --- Type E: p_frac = total p-orbital % ---\n",
    "    desc['E_pfrac_VBM'] = sum(raw[el]['p_VBM'] for el in elements)\n",
    "    desc['E_pfrac_CBM'] = sum(raw[el]['p_CBM'] for el in elements)\n",
    "    \n",
    "    # --- Type F: p_heavy = p_frac_of_heaviest * M_heaviest ---\n",
    "    if heavy1:\n",
    "        desc['F_ph1_VBM'] = raw[heavy1]['p_VBM'] * raw[heavy1]['mass']\n",
    "        desc['F_ph1_CBM'] = raw[heavy1]['p_CBM'] * raw[heavy1]['mass']\n",
    "    else:\n",
    "        desc['F_ph1_VBM'] = 0\n",
    "        desc['F_ph1_CBM'] = 0\n",
    "    \n",
    "    if heavy2:\n",
    "        desc['F_ph2_VBM'] = raw[heavy2]['p_VBM'] * raw[heavy2]['mass']\n",
    "        desc['F_ph2_CBM'] = raw[heavy2]['p_CBM'] * raw[heavy2]['mass']\n",
    "    else:\n",
    "        desc['F_ph2_VBM'] = 0\n",
    "        desc['F_ph2_CBM'] = 0\n",
    "    \n",
    "    # --- Metadata ---\n",
    "    desc['heavy1_el'] = heavy1 if heavy1 else 'NA'\n",
    "    desc['heavy2_el'] = heavy2 if heavy2 else 'NA'\n",
    "    desc['heavy1_mass'] = raw[heavy1]['mass'] if heavy1 else 0\n",
    "    desc['heavy2_mass'] = raw[heavy2]['mass'] if heavy2 else 0\n",
    "    desc['n_elements'] = len(elements)\n",
    "    \n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BATCH PROCESS ALL COMPOUNDS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {w: [] for w in WINDOWS}\n",
    "failed = []\n",
    "\n",
    "for idx, row in target.iterrows():\n",
    "    uid = row['uid']\n",
    "    formula = row['Formula']\n",
    "    alpha_R = row['alpha_R_max']\n",
    "    \n",
    "    if uid not in compound_dirs:\n",
    "        failed.append({'uid': uid, 'formula': formula, 'reason': 'no_folder'})\n",
    "        continue\n",
    "    \n",
    "    vasprun_path = compound_dirs[uid]['vasprun']\n",
    "    \n",
    "    print(f\"[{idx+1}/{len(target)}] {formula} ({uid[:12]}...)\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        for w in WINDOWS:\n",
    "            raw = extract_contributions(vasprun_path, w)\n",
    "            desc = compute_descriptors(raw)\n",
    "            desc['uid'] = uid\n",
    "            desc['Formula'] = formula\n",
    "            desc['alpha_R'] = alpha_R\n",
    "            desc['window'] = w\n",
    "            all_results[w].append(desc)\n",
    "        print(\"OK\")\n",
    "    except Exception as e:\n",
    "        failed.append({'uid': uid, 'formula': formula, 'reason': str(e)[:80]})\n",
    "        print(f\"FAILED: {str(e)[:60]}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Processed: {len(all_results[WINDOWS[0]])} compounds\")\n",
    "print(f\"Failed: {len(failed)}\")\n",
    "if failed:\n",
    "    print(\"Failed compounds:\")\n",
    "    for f in failed[:10]:\n",
    "        print(f\"  {f['formula']} ({f['uid'][:12]}): {f['reason']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUILD DATAFRAMES & SAVE CSVs\n",
    "# ============================================================\n",
    "\n",
    "# Descriptor type -> column prefixes\n",
    "type_cols = {\n",
    "    'A': [c for c in ['A_WM_VBM', 'A_WM_CBM']],\n",
    "    'B': [c for c in ['B_WMp_VBM', 'B_WMp_CBM']],\n",
    "    'C': [c for c in ['C_pfrac_h1_VBM', 'C_pfrac_h1_CBM', 'C_pfrac_h2_VBM', 'C_pfrac_h2_CBM']],\n",
    "    'D': [c for c in ['D_wm_h1_VBM', 'D_wm_h1_CBM', 'D_wm_h2_VBM', 'D_wm_h2_CBM']],\n",
    "    'E': [c for c in ['E_pfrac_VBM', 'E_pfrac_CBM']],\n",
    "    'F': [c for c in ['F_ph1_VBM', 'F_ph1_CBM', 'F_ph2_VBM', 'F_ph2_CBM']],\n",
    "}\n",
    "meta_cols = ['uid', 'Formula', 'alpha_R', 'heavy1_el', 'heavy2_el', 'heavy1_mass', 'heavy2_mass', 'n_elements']\n",
    "\n",
    "saved_csvs = []\n",
    "\n",
    "for w in WINDOWS:\n",
    "    df = pd.DataFrame(all_results[w])\n",
    "    w_str = str(w).replace('.', '')\n",
    "    \n",
    "    # Individual type CSVs\n",
    "    for t_name, t_cols in type_cols.items():\n",
    "        cols = meta_cols + t_cols\n",
    "        csv_name = f\"desc_{t_name}_w{w_str}.csv\"\n",
    "        csv_path = os.path.join(OUTPUT_DIR, csv_name)\n",
    "        df[cols].to_csv(csv_path, index=False)\n",
    "        saved_csvs.append(csv_name)\n",
    "    \n",
    "    # Combined: all types for this window\n",
    "    all_feature_cols = meta_cols + [c for cols in type_cols.values() for c in cols]\n",
    "    csv_name = f\"desc_ALL_w{w_str}.csv\"\n",
    "    csv_path = os.path.join(OUTPUT_DIR, csv_name)\n",
    "    df[all_feature_cols].to_csv(csv_path, index=False)\n",
    "    saved_csvs.append(csv_name)\n",
    "\n",
    "# Also: some useful combos across windows\n",
    "# Combo 1: Type A across all windows\n",
    "combo_rows = []\n",
    "for w in WINDOWS:\n",
    "    df_w = pd.DataFrame(all_results[w])\n",
    "    w_str = str(w).replace('.', '')\n",
    "    rename = {c: f\"{c}_w{w_str}\" for c in ['A_WM_VBM','A_WM_CBM','B_WMp_VBM','B_WMp_CBM',\n",
    "              'E_pfrac_VBM','E_pfrac_CBM','D_wm_h1_VBM','D_wm_h1_CBM',\n",
    "              'F_ph1_VBM','F_ph1_CBM']}\n",
    "    for old, new in rename.items():\n",
    "        if old in df_w.columns:\n",
    "            df_w[new] = df_w[old]\n",
    "\n",
    "# Multi-window combined\n",
    "dfs_w = {}\n",
    "for w in WINDOWS:\n",
    "    w_str = str(w).replace('.', '')\n",
    "    df_w = pd.DataFrame(all_results[w])\n",
    "    feature_cols_w = [c for cols in type_cols.values() for c in cols]\n",
    "    df_w = df_w[['uid', 'Formula', 'alpha_R'] + feature_cols_w]\n",
    "    df_w = df_w.rename(columns={c: f\"{c}_w{w_str}\" for c in feature_cols_w})\n",
    "    dfs_w[w] = df_w\n",
    "\n",
    "# Merge all windows\n",
    "df_multi = dfs_w[WINDOWS[0]]\n",
    "for w in WINDOWS[1:]:\n",
    "    df_multi = df_multi.merge(dfs_w[w].drop(columns=['Formula', 'alpha_R']), on='uid')\n",
    "\n",
    "csv_name = \"desc_ALL_multiwindow.csv\"\n",
    "csv_path = os.path.join(OUTPUT_DIR, csv_name)\n",
    "df_multi.to_csv(csv_path, index=False)\n",
    "saved_csvs.append(csv_name)\n",
    "\n",
    "print(f\"\\nSaved {len(saved_csvs)} CSV files to {OUTPUT_DIR}:\")\n",
    "for name in sorted(saved_csvs):\n",
    "    fpath = os.path.join(OUTPUT_DIR, name)\n",
    "    size = os.path.getsize(fpath) if os.path.exists(fpath) else 0\n",
    "    print(f\"  {name} ({size/1024:.1f} KB)\")",
    "\n",
    "# --- Extra: Type E combo CSVs (best performer) ---\n",
    "# E with 1.0 eV window\n",
    "if 1.0 in WINDOWS:\n",
    "    df_e10 = pd.DataFrame(all_results[1.0])\n",
    "    e10_path = os.path.join(OUTPUT_DIR, 'desc_E_w10.csv')\n",
    "    df_e10[meta_cols + ['E_pfrac_VBM', 'E_pfrac_CBM']].to_csv(e10_path, index=False)\n",
    "    saved_csvs.append('desc_E_w10.csv')\n",
    "\n",
    "# E with 0.5 + 1.0 combined (4 features)\n",
    "if 0.5 in WINDOWS and 1.0 in WINDOWS:\n",
    "    df_e05 = pd.DataFrame(all_results[0.5])[['uid', 'Formula', 'alpha_R', 'E_pfrac_VBM', 'E_pfrac_CBM']]\n",
    "    df_e05 = df_e05.rename(columns={'E_pfrac_VBM': 'E_pfrac_VBM_w05', 'E_pfrac_CBM': 'E_pfrac_CBM_w05'})\n",
    "    df_e10_slim = pd.DataFrame(all_results[1.0])[['uid', 'E_pfrac_VBM', 'E_pfrac_CBM']]\n",
    "    df_e10_slim = df_e10_slim.rename(columns={'E_pfrac_VBM': 'E_pfrac_VBM_w10', 'E_pfrac_CBM': 'E_pfrac_CBM_w10'})\n",
    "    df_e_combo = df_e05.merge(df_e10_slim, on='uid')\n",
    "    combo_path = os.path.join(OUTPUT_DIR, 'desc_E_w05_w10.csv')\n",
    "    df_e_combo.to_csv(combo_path, index=False)\n",
    "    saved_csvs.append('desc_E_w05_w10.csv')\n",
    "    print(f'\\nE combo (0.5+1.0) shape: {df_e_combo.shape}')\n",
    "    print(df_e_combo.head())\n",
    "\n",
    "# ============================================================\n",
    "# ENHANCED E VARIANTS: p_frac + elemental properties\n",
    "# ============================================================\n",
    "# Key elemental properties for SOC/Rashba:\n",
    "# - Z^4: SOC scales as Z^4 (THE most physically relevant)\n",
    "# - atomic_mass: heavier = more SOC\n",
    "# - atomic_radius: larger atoms = more diffuse orbitals\n",
    "# - electronegativity (X): controls charge transfer / dipole\n",
    "# - ionization_energy: relates to orbital energy levels\n",
    "#\n",
    "# For each compound, we compute:\n",
    "# - max_Z4: Z^4 of heaviest element\n",
    "# - weighted_Z4: sum(w_X * Z_X^4) at VBM/CBM\n",
    "# - max_mass: mass of heaviest element\n",
    "# - WM_VBM/CBM: sum(w_X * M_X)\n",
    "# - electronegativity_diff: max(X) - min(X) across elements\n",
    "# - weighted_radius: sum(w_X * r_X)\n",
    "\n",
    "from pymatgen.core.periodic_table import Element as Elem\n",
    "\n",
    "def compute_elemental_features(raw_contrib):\n",
    "    \"\"\"Compute elemental property descriptors from raw contributions.\"\"\"\n",
    "    feat = {}\n",
    "    elements = list(raw_contrib.keys())\n",
    "    \n",
    "    # Gather properties\n",
    "    props = {}\n",
    "    for el in elements:\n",
    "        e = Elem(el)\n",
    "        props[el] = {\n",
    "            'Z': e.Z,\n",
    "            'Z4': e.Z ** 4,\n",
    "            'mass': float(e.atomic_mass),\n",
    "            'radius': float(e.atomic_radius) if e.atomic_radius else 1.0,\n",
    "            'X': float(e.X) if e.X else 2.0,\n",
    "            'IE': float(e.ionization_energy) if e.ionization_energy else 8.0,\n",
    "            'vdw': float(e.van_der_waals_radius) if e.van_der_waals_radius else 1.5,\n",
    "        }\n",
    "    \n",
    "    # --- Raw elemental (no DOS weighting) ---\n",
    "    masses = [props[el]['mass'] for el in elements]\n",
    "    feat['max_mass'] = max(masses)\n",
    "    feat['max_Z'] = max(props[el]['Z'] for el in elements)\n",
    "    feat['max_Z4'] = max(props[el]['Z4'] for el in elements)\n",
    "    Xs = [props[el]['X'] for el in elements]\n",
    "    feat['X_diff'] = max(Xs) - min(Xs)  # electronegativity difference\n",
    "    feat['X_mean'] = np.mean(Xs)\n",
    "    radii = [props[el]['radius'] for el in elements]\n",
    "    feat['radius_diff'] = max(radii) - min(radii)\n",
    "    feat['radius_mean'] = np.mean(radii)\n",
    "    \n",
    "    # --- DOS-weighted elemental ---\n",
    "    for band in ['VBM', 'CBM']:\n",
    "        w_key = f'w_{band}'\n",
    "        p_key = f'p_{band}'\n",
    "        \n",
    "        # Weighted mass (WM)\n",
    "        feat[f'WM_{band}'] = sum(raw_contrib[el][w_key] * props[el]['mass'] for el in elements)\n",
    "        \n",
    "        # Weighted Z^4 (THE key one for SOC)\n",
    "        feat[f'WZ4_{band}'] = sum(raw_contrib[el][w_key] * props[el]['Z4'] for el in elements)\n",
    "        \n",
    "        # p-weighted Z^4\n",
    "        feat[f'pZ4_{band}'] = sum(raw_contrib[el][p_key] * props[el]['Z4'] for el in elements)\n",
    "        \n",
    "        # Weighted radius\n",
    "        feat[f'Wr_{band}'] = sum(raw_contrib[el][w_key] * props[el]['radius'] for el in elements)\n",
    "        \n",
    "        # Weighted electronegativity\n",
    "        feat[f'WX_{band}'] = sum(raw_contrib[el][w_key] * props[el]['X'] for el in elements)\n",
    "    \n",
    "    return feat\n",
    "\n",
    "# Recompute for 0.5 and 1.0 windows with elemental features\n",
    "print('\\nComputing enhanced E variants with elemental properties...')\n",
    "\n",
    "for w, w_str in [(0.5, 'w05'), (1.0, 'w10')]:\n",
    "    if w not in all_results or not all_results[w]:\n",
    "        continue\n",
    "    enhanced_rows = []\n",
    "    for entry in all_results[w]:\n",
    "        uid = entry['uid']\n",
    "        if uid not in compound_dirs:\n",
    "            continue\n",
    "        try:\n",
    "            raw = extract_contributions(compound_dirs[uid]['vasprun'], w)\n",
    "            elem_feat = compute_elemental_features(raw)\n",
    "            row = {\n",
    "                'uid': uid,\n",
    "                'Formula': entry['Formula'],\n",
    "                'alpha_R': entry['alpha_R'],\n",
    "                'E_pfrac_VBM': entry['E_pfrac_VBM'],\n",
    "                'E_pfrac_CBM': entry['E_pfrac_CBM'],\n",
    "            }\n",
    "            row.update(elem_feat)\n",
    "            enhanced_rows.append(row)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    df_enh = pd.DataFrame(enhanced_rows)\n",
    "    csv_name = f'desc_E_enhanced_{w_str}.csv'\n",
    "    df_enh.to_csv(os.path.join(OUTPUT_DIR, csv_name), index=False)\n",
    "    saved_csvs.append(csv_name)\n",
    "    print(f'  {csv_name}: {df_enh.shape}')\n",
    "\n",
    "# Combined 0.5 + 1.0 enhanced\n",
    "if 0.5 in all_results and 1.0 in all_results:\n",
    "    df_05 = pd.read_csv(os.path.join(OUTPUT_DIR, 'desc_E_enhanced_w05.csv'))\n",
    "    df_10 = pd.read_csv(os.path.join(OUTPUT_DIR, 'desc_E_enhanced_w10.csv'))\n",
    "    # Rename to avoid collision\n",
    "    rename_05 = {c: f'{c}_w05' for c in df_05.columns if c not in ['uid','Formula','alpha_R','max_mass','max_Z','max_Z4','X_diff','X_mean','radius_diff','radius_mean']}\n",
    "    rename_10 = {c: f'{c}_w10' for c in df_10.columns if c not in ['uid','Formula','alpha_R','max_mass','max_Z','max_Z4','X_diff','X_mean','radius_diff','radius_mean']}\n",
    "    df_05r = df_05.rename(columns=rename_05)\n",
    "    df_10r = df_10.rename(columns=rename_10)\n",
    "    # Merge\n",
    "    shared = ['uid','Formula','alpha_R','max_mass','max_Z','max_Z4','X_diff','X_mean','radius_diff','radius_mean']\n",
    "    df_combo = df_05r.merge(df_10r.drop(columns=[c for c in shared if c in df_10r.columns and c != 'uid']), on='uid')\n",
    "    csv_name = 'desc_E_enhanced_w05_w10.csv'\n",
    "    df_combo.to_csv(os.path.join(OUTPUT_DIR, csv_name), index=False)\n",
    "    saved_csvs.append(csv_name)\n",
    "    print(f'  {csv_name}: {df_combo.shape}')\n",
    "\n",
    "print(f'\\nTotal CSVs saved: {len(saved_csvs)}')\n",
    "for name in sorted(saved_csvs):\n",
    "    print(f'  {name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK SANITY CHECK\n",
    "# ============================================================\n",
    "df_check = pd.read_csv(os.path.join(OUTPUT_DIR, f\"desc_ALL_w005.csv\"))\n",
    "print(f\"Shape: {df_check.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df_check.head())\n",
    "print(f\"\\nDescriptor stats:\")\n",
    "feature_cols = [c for c in df_check.columns if c.startswith(('A_','B_','C_','D_','E_','F_'))]\n",
    "print(df_check[feature_cols].describe().round(3))\n",
    "print(f\"\\nalpha_R range: [{df_check['alpha_R'].min():.3f}, {df_check['alpha_R'].max():.3f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}