{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7b186b",
   "metadata": {},
   "source": [
    "# NB4: Classification - Predicting Strong Rashba Splitting\n",
    "\n",
    "**Goal:** Classify 2D materials as having strong vs weak Rashba splitting (alpha_R > threshold).\n",
    "\n",
    "Uses the same best_combo CSVs from nb2/nb3. Tests multiple classifiers with grid search,\n",
    "multiple thresholds, stratified K-fold + LOO, and handles class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777293dc",
   "metadata": {},
   "source": [
    "## Cell 1: Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, LeaveOneOut, GridSearchCV, cross_val_predict\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 11})\n",
    "\n",
    "# Paths - adjust if needed\n",
    "STUDY_DIR = \".\"  # same folder where this notebook lives, with best_combo CSVs\n",
    "# If CSVs are somewhere else, change this path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e49145",
   "metadata": {},
   "source": [
    "## Cell 2: Load All CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5201114",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = sorted(glob.glob(os.path.join(STUDY_DIR, \"best_combo_*.csv\")))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "\n",
    "datasets = {}\n",
    "for f in csv_files:\n",
    "    name = os.path.splitext(os.path.basename(f))[0]\n",
    "    df = pd.read_csv(f)\n",
    "    datasets[name] = df\n",
    "    print(f\"  {name}: {df.shape[0]} rows, {df.shape[1]} cols -> {list(df.columns)}\")\n",
    "\n",
    "# Quick check: show alpha_R distribution\n",
    "sample_df = list(datasets.values())[0]\n",
    "print(f\"\\nalpha_R stats:\\n{sample_df['alpha_R'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e164f",
   "metadata": {},
   "source": [
    "## Cell 3: alpha_R Distribution & Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09114bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(sample_df['alpha_R'], bins=25, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('alpha_R (eV*A)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of alpha_R')\n",
    "\n",
    "# Threshold analysis\n",
    "thresholds = [1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "class_counts = []\n",
    "for t in thresholds:\n",
    "    n_pos = (sample_df['alpha_R'] >= t).sum()\n",
    "    n_neg = (sample_df['alpha_R'] < t).sum()\n",
    "    ratio = n_pos / len(sample_df) * 100\n",
    "    class_counts.append({'threshold': t, 'positive': n_pos, 'negative': n_neg, 'pct_positive': ratio})\n",
    "    axes[0].axvline(x=t, color='red', linestyle='--', alpha=0.5, label=f't={t}')\n",
    "\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# Class balance bar chart\n",
    "cc_df = pd.DataFrame(class_counts)\n",
    "x = np.arange(len(thresholds))\n",
    "w = 0.35\n",
    "axes[1].bar(x - w/2, cc_df['negative'], w, label='Class 0 (weak)', color='#4C72B0')\n",
    "axes[1].bar(x + w/2, cc_df['positive'], w, label='Class 1 (strong)', color='#DD8452')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([str(t) for t in thresholds])\n",
    "axes[1].set_xlabel('Threshold (eV*A)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Class Balance at Different Thresholds')\n",
    "axes[1].legend()\n",
    "for i, row in cc_df.iterrows():\n",
    "    axes[1].text(i + w/2, row['positive'] + 1, f\"{row['pct_positive']:.0f}%\", ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass balance summary:\")\n",
    "print(cc_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fcd90e",
   "metadata": {},
   "source": [
    "## Cell 4: Define Models & Grid Search Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fec8b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_models_and_params():\n",
    "    \"\"\"\n",
    "    Returns dict of model_name -> (estimator, param_grid).\n",
    "    Grids are kept small enough to run on n=99 without taking forever.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': (\n",
    "            LogisticRegression(max_iter=5000, random_state=42),\n",
    "            {\n",
    "                'clf__C': [0.01, 0.1, 1.0, 10.0],\n",
    "                'clf__penalty': ['l1', 'l2'],\n",
    "                'clf__solver': ['saga'],\n",
    "                'clf__class_weight': [None, 'balanced']\n",
    "            }\n",
    "        ),\n",
    "        'SVM_RBF': (\n",
    "            SVC(probability=True, random_state=42),\n",
    "            {\n",
    "                'clf__C': [0.1, 1.0, 10.0, 100.0],\n",
    "                'clf__gamma': ['scale', 'auto', 0.1],\n",
    "                'clf__kernel': ['rbf'],\n",
    "                'clf__class_weight': [None, 'balanced']\n",
    "            }\n",
    "        ),\n",
    "        'SVM_Linear': (\n",
    "            SVC(probability=True, random_state=42),\n",
    "            {\n",
    "                'clf__C': [0.01, 0.1, 1.0, 10.0],\n",
    "                'clf__kernel': ['linear'],\n",
    "                'clf__class_weight': [None, 'balanced']\n",
    "            }\n",
    "        ),\n",
    "        'KNN': (\n",
    "            KNeighborsClassifier(),\n",
    "            {\n",
    "                'clf__n_neighbors': [3, 5, 7, 9, 11],\n",
    "                'clf__weights': ['uniform', 'distance'],\n",
    "                'clf__metric': ['euclidean', 'manhattan']\n",
    "            }\n",
    "        ),\n",
    "        'DecisionTree': (\n",
    "            DecisionTreeClassifier(random_state=42),\n",
    "            {\n",
    "                'clf__max_depth': [2, 3, 5, 7, None],\n",
    "                'clf__min_samples_split': [2, 5, 10],\n",
    "                'clf__class_weight': [None, 'balanced']\n",
    "            }\n",
    "        ),\n",
    "        'RandomForest': (\n",
    "            RandomForestClassifier(random_state=42),\n",
    "            {\n",
    "                'clf__n_estimators': [50, 100, 200],\n",
    "                'clf__max_depth': [3, 5, 7, None],\n",
    "                'clf__min_samples_split': [2, 5],\n",
    "                'clf__class_weight': [None, 'balanced']\n",
    "            }\n",
    "        ),\n",
    "        'GradientBoosting': (\n",
    "            GradientBoostingClassifier(random_state=42),\n",
    "            {\n",
    "                'clf__n_estimators': [50, 100, 200],\n",
    "                'clf__max_depth': [2, 3, 5],\n",
    "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
    "                'clf__subsample': [0.8, 1.0]\n",
    "            }\n",
    "        ),\n",
    "        'XGBoost': (\n",
    "            XGBClassifier(\n",
    "                eval_metric='logloss', use_label_encoder=False,\n",
    "                random_state=42, verbosity=0\n",
    "            ),\n",
    "            {\n",
    "                'clf__n_estimators': [50, 100, 200],\n",
    "                'clf__max_depth': [2, 3, 5],\n",
    "                'clf__learning_rate': [0.01, 0.1, 0.2],\n",
    "                'clf__reg_alpha': [0, 1.0],\n",
    "                'clf__reg_lambda': [1.0],\n",
    "                'clf__scale_pos_weight': [1, 3, 5]\n",
    "            }\n",
    "        ),\n",
    "        'AdaBoost': (\n",
    "            AdaBoostClassifier(random_state=42, algorithm='SAMME'),\n",
    "            {\n",
    "                'clf__n_estimators': [50, 100, 200],\n",
    "                'clf__learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    "    return models\n",
    "\n",
    "print(\"Models defined:\")\n",
    "for name, (est, params) in get_models_and_params().items():\n",
    "    n_combos = 1\n",
    "    for v in params.values():\n",
    "        n_combos *= len(v)\n",
    "    print(f\"  {name}: {n_combos} grid combos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54733e",
   "metadata": {},
   "source": [
    "## Cell 5: Core Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0da81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(X, y, model_name, estimator, param_grid, n_splits=5):\n",
    "    \"\"\"\n",
    "    Run stratified K-fold GridSearchCV + LOO evaluation.\n",
    "    Returns dict with all metrics.\n",
    "    \"\"\"\n",
    "    # Build pipeline with scaler\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', estimator)\n",
    "    ])\n",
    "\n",
    "    # --- Stratified K-Fold with GridSearchCV ---\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe, param_grid, cv=skf, scoring='f1',\n",
    "        n_jobs=-1, refit=True, error_score=0.0\n",
    "    )\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    best_pipe = grid.best_estimator_\n",
    "    best_params = grid.best_params_\n",
    "\n",
    "    # K-Fold predictions using cross_val_predict with best params\n",
    "    y_pred_kfold = cross_val_predict(best_pipe, X, y, cv=skf)\n",
    "\n",
    "    # Get probabilities if possible\n",
    "    try:\n",
    "        y_prob_kfold = cross_val_predict(best_pipe, X, y, cv=skf, method='predict_proba')[:, 1]\n",
    "        kfold_auc = roc_auc_score(y, y_prob_kfold)\n",
    "    except Exception:\n",
    "        y_prob_kfold = None\n",
    "        kfold_auc = np.nan\n",
    "\n",
    "    kfold_acc = accuracy_score(y, y_pred_kfold)\n",
    "    kfold_prec = precision_score(y, y_pred_kfold, zero_division=0)\n",
    "    kfold_rec = recall_score(y, y_pred_kfold, zero_division=0)\n",
    "    kfold_f1 = f1_score(y, y_pred_kfold, zero_division=0)\n",
    "\n",
    "    # --- Leave-One-Out ---\n",
    "    loo = LeaveOneOut()\n",
    "    y_pred_loo = cross_val_predict(best_pipe, X, y, cv=loo)\n",
    "\n",
    "    try:\n",
    "        y_prob_loo = cross_val_predict(best_pipe, X, y, cv=loo, method='predict_proba')[:, 1]\n",
    "        loo_auc = roc_auc_score(y, y_prob_loo)\n",
    "    except Exception:\n",
    "        y_prob_loo = None\n",
    "        loo_auc = np.nan\n",
    "\n",
    "    loo_acc = accuracy_score(y, y_pred_loo)\n",
    "    loo_prec = precision_score(y, y_pred_loo, zero_division=0)\n",
    "    loo_rec = recall_score(y, y_pred_loo, zero_division=0)\n",
    "    loo_f1 = f1_score(y, y_pred_loo, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_params': str(best_params),\n",
    "        'KF_Acc': kfold_acc, 'KF_Prec': kfold_prec,\n",
    "        'KF_Rec': kfold_rec, 'KF_F1': kfold_f1, 'KF_AUC': kfold_auc,\n",
    "        'LOO_Acc': loo_acc, 'LOO_Prec': loo_prec,\n",
    "        'LOO_Rec': loo_rec, 'LOO_F1': loo_f1, 'LOO_AUC': loo_auc,\n",
    "        'y_true': y, 'y_pred_kfold': y_pred_kfold, 'y_prob_kfold': y_prob_kfold,\n",
    "        'y_pred_loo': y_pred_loo, 'y_prob_loo': y_prob_loo\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb830579",
   "metadata": {},
   "source": [
    "## Cell 6: Run All Models x All CSVs x All Thresholds\n",
    "\n",
    "This is the big loop. For each threshold, for each CSV, for each model: GridSearchCV + LOO.\n",
    "Takes a while on 99 samples but manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_to_run = [1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "NON_FEATURE_COLS = ['uid', 'Formula', 'alpha_R']\n",
    "\n",
    "all_results = []\n",
    "detailed_results = {}  # store predictions for later plotting\n",
    "\n",
    "for thresh in thresholds_to_run:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  THRESHOLD = {thresh} eV*A\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    for csv_name, df in datasets.items():\n",
    "        # Prepare features and binary target\n",
    "        feature_cols = [c for c in df.columns if c not in NON_FEATURE_COLS]\n",
    "        X = df[feature_cols].values\n",
    "        y_cont = df['alpha_R'].values\n",
    "        y = (y_cont >= thresh).astype(int)\n",
    "\n",
    "        n_pos = y.sum()\n",
    "        n_neg = len(y) - n_pos\n",
    "        print(f\"\\n  {csv_name} | Class 1: {n_pos}, Class 0: {n_neg}\")\n",
    "\n",
    "        # Skip if too few in minority class (need at least 5 for stratified 5-fold)\n",
    "        if min(n_pos, n_neg) < 5:\n",
    "            print(f\"    SKIPPED: minority class has <5 samples\")\n",
    "            continue\n",
    "\n",
    "        models = get_models_and_params()\n",
    "        for model_name, (estimator, param_grid) in models.items():\n",
    "            print(f\"    Running {model_name}...\", end=\" \", flush=True)\n",
    "            try:\n",
    "                res = evaluate_classification(X, y, model_name, estimator, param_grid)\n",
    "                res['csv'] = csv_name\n",
    "                res['threshold'] = thresh\n",
    "                res['n_features'] = len(feature_cols)\n",
    "                res['n_pos'] = n_pos\n",
    "                res['n_neg'] = n_neg\n",
    "\n",
    "                # Store detailed preds\n",
    "                key = (thresh, csv_name, model_name)\n",
    "                detailed_results[key] = {\n",
    "                    'y_true': res.pop('y_true'),\n",
    "                    'y_pred_kfold': res.pop('y_pred_kfold'),\n",
    "                    'y_prob_kfold': res.pop('y_prob_kfold'),\n",
    "                    'y_pred_loo': res.pop('y_pred_loo'),\n",
    "                    'y_prob_loo': res.pop('y_prob_loo')\n",
    "                }\n",
    "                all_results.append(res)\n",
    "                print(f\"LOO_F1={res['LOO_F1']:.3f}, LOO_AUC={res['LOO_AUC']:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"FAILED: {e}\")\n",
    "                continue\n",
    "\n",
    "print(f\"\\n\\nTotal results: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b075d",
   "metadata": {},
   "source": [
    "## Cell 7: Results DataFrame & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Sort by LOO F1\n",
    "df_results = df_results.sort_values('LOO_F1', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top 20\n",
    "display_cols = ['threshold', 'csv', 'model', 'KF_F1', 'KF_AUC', 'LOO_F1', 'LOO_AUC', 'LOO_Acc', 'LOO_Prec', 'LOO_Rec']\n",
    "print(\"Top 20 results by LOO F1:\")\n",
    "print(df_results[display_cols].head(20).to_string(index=False))\n",
    "\n",
    "# Save full results\n",
    "df_results.to_csv('classification_full_results.csv', index=False)\n",
    "print(f\"\\nSaved classification_full_results.csv ({len(df_results)} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eed945",
   "metadata": {},
   "source": [
    "## Cell 8: Best Model per Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8370382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model per threshold (by LOO F1):\\n\")\n",
    "best_per_thresh = df_results.loc[df_results.groupby('threshold')['LOO_F1'].idxmax()]\n",
    "print(best_per_thresh[['threshold', 'csv', 'model', 'KF_F1', 'KF_AUC',\n",
    "                        'LOO_F1', 'LOO_AUC', 'LOO_Acc', 'LOO_Prec', 'LOO_Rec',\n",
    "                        'best_params']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f992cc",
   "metadata": {},
   "source": [
    "## Cell 9: Best Model per CSV (across all thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best model per CSV (by LOO F1):\\n\")\n",
    "best_per_csv = df_results.loc[df_results.groupby('csv')['LOO_F1'].idxmax()]\n",
    "print(best_per_csv[['csv', 'threshold', 'model', 'KF_F1', 'KF_AUC',\n",
    "                     'LOO_F1', 'LOO_AUC', 'n_features']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6c9b5",
   "metadata": {},
   "source": [
    "## Cell 10: Heatmap - LOO F1 (Model x Threshold) for best CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62262ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the CSV that gave overall best LOO F1\n",
    "best_csv = df_results.iloc[0]['csv']\n",
    "print(f\"Heatmap for best CSV: {best_csv}\\n\")\n",
    "\n",
    "subset = df_results[df_results['csv'] == best_csv]\n",
    "pivot_f1 = subset.pivot_table(index='model', columns='threshold', values='LOO_F1', aggfunc='max')\n",
    "pivot_auc = subset.pivot_table(index='model', columns='threshold', values='LOO_AUC', aggfunc='max')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# F1 heatmap\n",
    "im1 = axes[0].imshow(pivot_f1.values, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "axes[0].set_xticks(range(len(pivot_f1.columns)))\n",
    "axes[0].set_xticklabels([f'{c:.1f}' for c in pivot_f1.columns])\n",
    "axes[0].set_yticks(range(len(pivot_f1.index)))\n",
    "axes[0].set_yticklabels(pivot_f1.index)\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_title(f'LOO F1 Score - {best_csv}')\n",
    "for i in range(len(pivot_f1.index)):\n",
    "    for j in range(len(pivot_f1.columns)):\n",
    "        val = pivot_f1.values[i, j]\n",
    "        if not np.isnan(val):\n",
    "            axes[0].text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=9,\n",
    "                        color='white' if val > 0.5 else 'black')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# AUC heatmap\n",
    "im2 = axes[1].imshow(pivot_auc.values, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1].set_xticks(range(len(pivot_auc.columns)))\n",
    "axes[1].set_xticklabels([f'{c:.1f}' for c in pivot_auc.columns])\n",
    "axes[1].set_yticks(range(len(pivot_auc.index)))\n",
    "axes[1].set_yticklabels(pivot_auc.index)\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_title(f'LOO AUC-ROC - {best_csv}')\n",
    "for i in range(len(pivot_auc.index)):\n",
    "    for j in range(len(pivot_auc.columns)):\n",
    "        val = pivot_auc.values[i, j]\n",
    "        if not np.isnan(val):\n",
    "            axes[1].text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=9,\n",
    "                        color='white' if val > 0.5 else 'black')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('heatmap_classification.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d92dd",
   "metadata": {},
   "source": [
    "## Cell 11: Confusion Matrices for Top 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top4 = df_results.head(4)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4.5))\n",
    "\n",
    "for idx, (_, row) in enumerate(top4.iterrows()):\n",
    "    key = (row['threshold'], row['csv'], row['model'])\n",
    "    det = detailed_results[key]\n",
    "\n",
    "    cm = confusion_matrix(det['y_true'], det['y_pred_loo'])\n",
    "    ConfusionMatrixDisplay(cm, display_labels=['Weak', 'Strong']).plot(ax=axes[idx], colorbar=False)\n",
    "    axes[idx].set_title(f\"{row['model']}\\nt={row['threshold']}, LOO_F1={row['LOO_F1']:.3f}\", fontsize=10)\n",
    "\n",
    "plt.suptitle('Confusion Matrices (LOO) - Top 4 Configurations', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices_top4.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245090c7",
   "metadata": {},
   "source": [
    "## Cell 12: ROC Curves for Top 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb6597",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for idx, (_, row) in enumerate(top4.iterrows()):\n",
    "    key = (row['threshold'], row['csv'], row['model'])\n",
    "    det = detailed_results[key]\n",
    "\n",
    "    if det['y_prob_loo'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(det['y_true'], det['y_prob_loo'])\n",
    "        roc_auc_val = auc(fpr, tpr)\n",
    "        label = f\"{row['model']} (t={row['threshold']}) AUC={roc_auc_val:.3f}\"\n",
    "        ax.plot(fpr, tpr, color=colors[idx], lw=2, label=label)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves (LOO) - Top 4 Configurations')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.set_xlim([-0.02, 1.02])\n",
    "ax.set_ylim([-0.02, 1.02])\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_top4.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cef7fa",
   "metadata": {},
   "source": [
    "## Cell 13: Threshold Comparison - Average LOO F1 across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598249ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_summary = df_results.groupby('threshold').agg(\n",
    "    mean_F1=('LOO_F1', 'mean'),\n",
    "    max_F1=('LOO_F1', 'max'),\n",
    "    mean_AUC=('LOO_AUC', 'mean'),\n",
    "    max_AUC=('LOO_AUC', 'max'),\n",
    "    mean_Acc=('LOO_Acc', 'mean'),\n",
    "    n_runs=('LOO_F1', 'count')\n",
    ").reset_index()\n",
    "\n",
    "print(\"Threshold comparison (averaged across all models and CSVs):\")\n",
    "print(thresh_summary.to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "x = np.arange(len(thresh_summary))\n",
    "w = 0.2\n",
    "ax.bar(x - w, thresh_summary['mean_F1'], w, label='Mean LOO F1', color='#4C72B0')\n",
    "ax.bar(x, thresh_summary['max_F1'], w, label='Max LOO F1', color='#DD8452')\n",
    "ax.bar(x + w, thresh_summary['max_AUC'], w, label='Max LOO AUC', color='#55A868')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{t:.1f}' for t in thresh_summary['threshold']])\n",
    "ax.set_xlabel('Threshold (eV*A)')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Classification Performance vs Threshold')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fb541",
   "metadata": {},
   "source": [
    "## Cell 14: KFold vs LOO Comparison (Top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = df_results.head(10).copy()\n",
    "top10['label'] = top10.apply(lambda r: f\"{r['model'][:8]}\\nt={r['threshold']}\", axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# F1 comparison\n",
    "x = np.arange(len(top10))\n",
    "w = 0.35\n",
    "axes[0].bar(x - w/2, top10['KF_F1'], w, label='5-Fold F1', color='#4C72B0')\n",
    "axes[0].bar(x + w/2, top10['LOO_F1'], w, label='LOO F1', color='#DD8452')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(top10['label'], rotation=45, ha='right', fontsize=8)\n",
    "axes[0].set_ylabel('F1 Score')\n",
    "axes[0].set_title('K-Fold vs LOO F1 (Top 10)')\n",
    "axes[0].legend()\n",
    "\n",
    "# AUC comparison\n",
    "axes[1].bar(x - w/2, top10['KF_AUC'], w, label='5-Fold AUC', color='#4C72B0')\n",
    "axes[1].bar(x + w/2, top10['LOO_AUC'], w, label='LOO AUC', color='#DD8452')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(top10['label'], rotation=45, ha='right', fontsize=8)\n",
    "axes[1].set_ylabel('AUC-ROC')\n",
    "axes[1].set_title('K-Fold vs LOO AUC (Top 10)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('kfold_vs_loo_classification.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cef20",
   "metadata": {},
   "source": [
    "## Cell 15: Detailed Report for Overall Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_row = df_results.iloc[0]\n",
    "best_key = (best_row['threshold'], best_row['csv'], best_row['model'])\n",
    "best_det = detailed_results[best_key]\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  BEST CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Model:     {best_row['model']}\")\n",
    "print(f\"  CSV:       {best_row['csv']}\")\n",
    "print(f\"  Threshold: {best_row['threshold']} eV*A\")\n",
    "print(f\"  Features:  {best_row['n_features']}\")\n",
    "print(f\"  Class balance: {best_row['n_pos']} positive, {best_row['n_neg']} negative\")\n",
    "print(f\"  Best params: {best_row['best_params']}\")\n",
    "print(f\"\\n  --- 5-Fold CV ---\")\n",
    "print(f\"  Accuracy:  {best_row['KF_Acc']:.4f}\")\n",
    "print(f\"  Precision: {best_row['KF_Prec']:.4f}\")\n",
    "print(f\"  Recall:    {best_row['KF_Rec']:.4f}\")\n",
    "print(f\"  F1:        {best_row['KF_F1']:.4f}\")\n",
    "print(f\"  AUC-ROC:   {best_row['KF_AUC']:.4f}\")\n",
    "print(f\"\\n  --- Leave-One-Out ---\")\n",
    "print(f\"  Accuracy:  {best_row['LOO_Acc']:.4f}\")\n",
    "print(f\"  Precision: {best_row['LOO_Prec']:.4f}\")\n",
    "print(f\"  Recall:    {best_row['LOO_Rec']:.4f}\")\n",
    "print(f\"  F1:        {best_row['LOO_F1']:.4f}\")\n",
    "print(f\"  AUC-ROC:   {best_row['LOO_AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (LOO):\")\n",
    "print(classification_report(best_det['y_true'], best_det['y_pred_loo'],\n",
    "                            target_names=['Weak Rashba', 'Strong Rashba']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd66ce",
   "metadata": {},
   "source": [
    "## Cell 16: Feature Importance for Best Tree-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best tree-based model for feature importance\n",
    "tree_models = ['RandomForest', 'XGBoost', 'GradientBoosting', 'DecisionTree', 'AdaBoost']\n",
    "tree_results = df_results[df_results['model'].isin(tree_models)]\n",
    "\n",
    "if len(tree_results) > 0:\n",
    "    best_tree = tree_results.iloc[0]\n",
    "    best_tree_key = (best_tree['threshold'], best_tree['csv'], best_tree['model'])\n",
    "\n",
    "    # Retrain the best tree model on full data to get feature importance\n",
    "    csv_df = datasets[best_tree['csv']]\n",
    "    feature_cols = [c for c in csv_df.columns if c not in NON_FEATURE_COLS]\n",
    "    X_full = csv_df[feature_cols].values\n",
    "    y_full = (csv_df['alpha_R'].values >= best_tree['threshold']).astype(int)\n",
    "\n",
    "    # Parse best params back\n",
    "    bp = eval(best_tree['best_params'])\n",
    "    # Rebuild the model with best params\n",
    "    models = get_models_and_params()\n",
    "    est, _ = models[best_tree['model']]\n",
    "    for k, v in bp.items():\n",
    "        param_name = k.replace('clf__', '')\n",
    "        if hasattr(est, param_name):\n",
    "            setattr(est, param_name, v)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_full)\n",
    "    est.fit(X_scaled, y_full)\n",
    "\n",
    "    importances = est.feature_importances_\n",
    "    feat_imp = pd.DataFrame({'feature': feature_cols, 'importance': importances})\n",
    "    feat_imp = feat_imp.sort_values('importance', ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, max(4, len(feature_cols) * 0.5)))\n",
    "    ax.barh(feat_imp['feature'], feat_imp['importance'], color='steelblue')\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title(f'Feature Importance - {best_tree[\"model\"]} (t={best_tree[\"threshold\"]})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_classification.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nFeature importances:\")\n",
    "    print(feat_imp.sort_values('importance', ascending=False).to_string(index=False))\n",
    "else:\n",
    "    print(\"No tree-based models in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27446d07",
   "metadata": {},
   "source": [
    "## Cell 17: Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nBest per threshold:\")\n",
    "for _, row in best_per_thresh.iterrows():\n",
    "    print(f\"  t={row['threshold']:.1f}: {row['model']} on {row['csv']} \"\n",
    "          f\"-> LOO_F1={row['LOO_F1']:.3f}, LOO_AUC={row['LOO_AUC']:.3f}\")\n",
    "\n",
    "print(f\"\\nOverall best: {df_results.iloc[0]['model']} at threshold {df_results.iloc[0]['threshold']} \"\n",
    "      f\"on {df_results.iloc[0]['csv']}\")\n",
    "print(f\"  LOO F1  = {df_results.iloc[0]['LOO_F1']:.4f}\")\n",
    "print(f\"  LOO AUC = {df_results.iloc[0]['LOO_AUC']:.4f}\")\n",
    "print(f\"  LOO Acc = {df_results.iloc[0]['LOO_Acc']:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal configurations tested: {len(df_results)}\")\n",
    "print(f\"Saved: classification_full_results.csv\")\n",
    "print(f\"Plots: heatmap_classification.png, confusion_matrices_top4.png,\")\n",
    "print(f\"       roc_curves_top4.png, threshold_comparison.png,\")\n",
    "print(f\"       kfold_vs_loo_classification.png, feature_importance_classification.png\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "executable": "/usr/bin/env python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
