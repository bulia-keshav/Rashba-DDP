{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Model Comparison Study\n",
    "Compares 10 regression models across the top descriptor sets.\n",
    "\n",
    "**Models:** Linear Regression, Ridge, Lasso, ElasticNet, SVR, KNN, Decision Tree, Random Forest, XGBoost, TabNet\n",
    "\n",
    "**Evaluation:** LOO R2, 5-Fold R2, MAE, RMSE + grid search tuning\n",
    "\n",
    "**Input:** best_combo_0 through best_combo_5 CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import KFold, LeaveOneOut, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing TabNet\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    import torch\n",
    "    TABNET_AVAILABLE = True\n",
    "    print('TabNet available')\n",
    "except ImportError:\n",
    "    TABNET_AVAILABLE = False\n",
    "    print('TabNet not available, will skip. Install: pip install pytorch-tabnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "BASE_DIR = r\"C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\"\n",
    "PARENT_DIR = os.path.join(BASE_DIR, \"Weight-contribution\", \"contribution-model\")\n",
    "STUDY_DIR = os.path.join(PARENT_DIR, \"best-descriptors-model-study\")\n",
    "os.makedirs(STUDY_DIR, exist_ok=True)\n",
    "\n",
    "# Load all best_combo CSVs\n",
    "csv_files = sorted(glob.glob(os.path.join(PARENT_DIR, \"best_combo_*.csv\")))\n",
    "print(f\"Found {len(csv_files)} CSVs:\")\n",
    "for f in csv_files:\n",
    "    name = os.path.basename(f)\n",
    "    df_tmp = pd.read_csv(f)\n",
    "    feat = [c for c in df_tmp.columns if c not in ['uid','Formula','alpha_R']]\n",
    "    print(f\"  {name}: {len(df_tmp)} rows, {len(feat)} features -> {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS WITH PARAM GRIDS\n",
    "# ============================================================\n",
    "NON_FEAT = {'uid', 'Formula', 'alpha_R'}\n",
    "\n",
    "def get_models():\n",
    "    models = {\n",
    "        'LinearReg': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {},  # no tuning needed\n",
    "            'scale': True,\n",
    "        },\n",
    "        'Ridge': {\n",
    "            'model': Ridge(),\n",
    "            'params': {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]},\n",
    "            'scale': True,\n",
    "        },\n",
    "        'Lasso': {\n",
    "            'model': Lasso(max_iter=5000),\n",
    "            'params': {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]},\n",
    "            'scale': True,\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'model': ElasticNet(max_iter=5000),\n",
    "            'params': {'alpha': [0.01, 0.1, 1.0], 'l1_ratio': [0.2, 0.5, 0.8]},\n",
    "            'scale': True,\n",
    "        },\n",
    "        'SVR': {\n",
    "            'model': SVR(),\n",
    "            'params': {'C': [0.1, 1, 10, 100], 'epsilon': [0.01, 0.1, 0.5], 'kernel': ['rbf', 'linear']},\n",
    "            'scale': True,\n",
    "        },\n",
    "        'KNN': {\n",
    "            'model': KNeighborsRegressor(),\n",
    "            'params': {'n_neighbors': [3, 5, 7, 9, 11, 15], 'weights': ['uniform', 'distance']},\n",
    "            'scale': True,\n",
    "        },\n",
    "        'DecisionTree': {\n",
    "            'model': DecisionTreeRegressor(random_state=42),\n",
    "            'params': {'max_depth': [2, 3, 4, 5, None], 'min_samples_split': [2, 5, 10]},\n",
    "            'scale': False,\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {'n_estimators': [50, 100, 200], 'max_depth': [2, 3, 4, None], 'min_samples_split': [2, 5]},\n",
    "            'scale': False,\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model': XGBRegressor(random_state=42, verbosity=0),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [2, 3, 4],\n",
    "                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                'reg_alpha': [0, 0.5, 1.0],\n",
    "                'reg_lambda': [0.5, 1.0, 2.0],\n",
    "            },\n",
    "            'scale': False,\n",
    "        },\n",
    "    }\n",
    "    return models\n",
    "\n",
    "print(f\"Models defined: {list(get_models().keys())}\")\n",
    "if TABNET_AVAILABLE:\n",
    "    print(\"+ TabNet (handled separately)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORE: TUNE + EVALUATE EACH MODEL\n",
    "# ============================================================\n",
    "\n",
    "def tune_and_evaluate(X, y, model_name, model_obj, param_grid, scale=True):\n",
    "    \"\"\"\n",
    "    1. Grid search with 5-fold CV to find best params\n",
    "    2. LOO with best params\n",
    "    3. 5-fold with best params\n",
    "    Returns dict with all metrics.\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    result = {'model': model_name}\n",
    "    \n",
    "    # --- Grid Search ---\n",
    "    if param_grid:\n",
    "        gs = GridSearchCV(model_obj, param_grid, cv=5, scoring='r2', n_jobs=-1, refit=True)\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            X_s = scaler.fit_transform(X)\n",
    "            gs.fit(X_s, y)\n",
    "        else:\n",
    "            gs.fit(X, y)\n",
    "        best_params = gs.best_params_\n",
    "        best_model = gs.best_estimator_\n",
    "        result['best_params'] = str(best_params)\n",
    "        result['grid_search_R2'] = gs.best_score_\n",
    "    else:\n",
    "        best_model = model_obj\n",
    "        best_params = {}\n",
    "        result['best_params'] = '{}'\n",
    "        result['grid_search_R2'] = np.nan\n",
    "    \n",
    "    # --- LOO ---\n",
    "    loo = LeaveOneOut()\n",
    "    y_pred_loo = np.zeros(n)\n",
    "    for train_idx, test_idx in loo.split(X):\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            X_tr = scaler.fit_transform(X[train_idx])\n",
    "            X_te = scaler.transform(X[test_idx])\n",
    "        else:\n",
    "            X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        best_model.fit(X_tr, y[train_idx])\n",
    "        y_pred_loo[test_idx] = best_model.predict(X_te)\n",
    "    \n",
    "    result['LOO_R2'] = r2_score(y, y_pred_loo)\n",
    "    result['LOO_MAE'] = mean_absolute_error(y, y_pred_loo)\n",
    "    result['LOO_RMSE'] = np.sqrt(mean_squared_error(y, y_pred_loo))\n",
    "    result['y_pred_loo'] = y_pred_loo  # keep for plotting\n",
    "    \n",
    "    # --- 5-Fold ---\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_pred_kf = np.zeros(n)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            X_tr = scaler.fit_transform(X[train_idx])\n",
    "            X_te = scaler.transform(X[test_idx])\n",
    "        else:\n",
    "            X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        best_model.fit(X_tr, y[train_idx])\n",
    "        y_pred_kf[test_idx] = best_model.predict(X_te)\n",
    "    \n",
    "    result['KFold_R2'] = r2_score(y, y_pred_kf)\n",
    "    result['KFold_MAE'] = mean_absolute_error(y, y_pred_kf)\n",
    "    \n",
    "    # --- Train R2 ---\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_s = scaler.fit_transform(X)\n",
    "        best_model.fit(X_s, y)\n",
    "        result['Train_R2'] = r2_score(y, best_model.predict(X_s))\n",
    "    else:\n",
    "        best_model.fit(X, y)\n",
    "        result['Train_R2'] = r2_score(y, best_model.predict(X))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def run_tabnet(X, y):\n",
    "    \"\"\"TabNet with LOO and KFold.\"\"\"\n",
    "    if not TABNET_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    n = len(y)\n",
    "    result = {'model': 'TabNet'}\n",
    "    \n",
    "    # LOO\n",
    "    y_pred_loo = np.zeros(n)\n",
    "    loo = LeaveOneOut()\n",
    "    for train_idx, test_idx in loo.split(X):\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = scaler.fit_transform(X[train_idx])\n",
    "        X_te = scaler.transform(X[test_idx])\n",
    "        \n",
    "        model = TabNetRegressor(\n",
    "            n_d=8, n_a=8, n_steps=3,\n",
    "            gamma=1.5, lambda_sparse=1e-3,\n",
    "            optimizer_params=dict(lr=0.02),\n",
    "            scheduler_params={\"step_size\":20, \"gamma\":0.9},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            verbose=0,\n",
    "            seed=42,\n",
    "        )\n",
    "        model.fit(\n",
    "            X_tr, y[train_idx].reshape(-1, 1),\n",
    "            eval_set=[(X_te, y[test_idx].reshape(-1, 1))],\n",
    "            max_epochs=200,\n",
    "            patience=30,\n",
    "            batch_size=32,\n",
    "        )\n",
    "        y_pred_loo[test_idx] = model.predict(X_te).flatten()\n",
    "    \n",
    "    result['LOO_R2'] = r2_score(y, y_pred_loo)\n",
    "    result['LOO_MAE'] = mean_absolute_error(y, y_pred_loo)\n",
    "    result['LOO_RMSE'] = np.sqrt(mean_squared_error(y, y_pred_loo))\n",
    "    result['y_pred_loo'] = y_pred_loo\n",
    "    \n",
    "    # KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    y_pred_kf = np.zeros(n)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = scaler.fit_transform(X[train_idx])\n",
    "        X_te = scaler.transform(X[test_idx])\n",
    "        model = TabNetRegressor(\n",
    "            n_d=8, n_a=8, n_steps=3,\n",
    "            gamma=1.5, lambda_sparse=1e-3,\n",
    "            optimizer_params=dict(lr=0.02),\n",
    "            scheduler_params={\"step_size\":20, \"gamma\":0.9},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            verbose=0,\n",
    "            seed=42,\n",
    "        )\n",
    "        model.fit(\n",
    "            X_tr, y[train_idx].reshape(-1, 1),\n",
    "            eval_set=[(X_te, y[test_idx].reshape(-1, 1))],\n",
    "            max_epochs=200,\n",
    "            patience=30,\n",
    "            batch_size=32,\n",
    "        )\n",
    "        y_pred_kf[test_idx] = model.predict(X_te).flatten()\n",
    "    \n",
    "    result['KFold_R2'] = r2_score(y, y_pred_kf)\n",
    "    result['KFold_MAE'] = mean_absolute_error(y, y_pred_kf)\n",
    "    result['Train_R2'] = np.nan  # TabNet overfits by design\n",
    "    result['best_params'] = 'n_d=8,n_a=8,n_steps=3,lr=0.02'\n",
    "    result['grid_search_R2'] = np.nan\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ALL MODELS ON ALL CSVs\n",
    "# ============================================================\n",
    "all_results = []\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    short_name = csv_name.replace('best_combo_','').replace('.csv','')\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    feat_cols = [c for c in df.columns if c not in NON_FEAT and df[c].dtype in ['float64','float32','int64','int32']]\n",
    "    X = df[feat_cols].values\n",
    "    y = df['alpha_R'].values\n",
    "    valid = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "    X, y = X[valid], y[valid]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CSV: {csv_name} | n={len(y)}, features={len(feat_cols)}: {feat_cols}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    models = get_models()\n",
    "    \n",
    "    for mname, mconfig in models.items():\n",
    "        t0 = time.time()\n",
    "        print(f\"  {mname:<15}\", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            res = tune_and_evaluate(X, y, mname, mconfig['model'], mconfig['params'], mconfig['scale'])\n",
    "            res['csv'] = csv_name\n",
    "            res['short_csv'] = short_name\n",
    "            res['n_features'] = len(feat_cols)\n",
    "            res['features'] = feat_cols\n",
    "            all_results.append(res)\n",
    "            dt = time.time() - t0\n",
    "            print(f\"LOO R2={res['LOO_R2']:.3f}  KF R2={res['KFold_R2']:.3f}  MAE={res['LOO_MAE']:.3f}  ({dt:.1f}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED: {str(e)[:60]}\")\n",
    "    \n",
    "    # TabNet\n",
    "    if TABNET_AVAILABLE:\n",
    "        t0 = time.time()\n",
    "        print(f\"  {'TabNet':<15}\", end=\"\", flush=True)\n",
    "        try:\n",
    "            res = run_tabnet(X, y)\n",
    "            if res:\n",
    "                res['csv'] = csv_name\n",
    "                res['short_csv'] = short_name\n",
    "                res['n_features'] = len(feat_cols)\n",
    "                res['features'] = feat_cols\n",
    "                all_results.append(res)\n",
    "                dt = time.time() - t0\n",
    "                print(f\"LOO R2={res['LOO_R2']:.3f}  KF R2={res['KFold_R2']:.3f}  MAE={res['LOO_MAE']:.3f}  ({dt:.1f}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED: {str(e)[:60]}\")\n",
    "\n",
    "print(f\"\\n\\nTotal runs: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MASTER RESULTS TABLE\n",
    "# ============================================================\n",
    "table_cols = ['csv', 'model', 'n_features', 'Train_R2', 'KFold_R2', 'LOO_R2', 'LOO_MAE', 'LOO_RMSE', 'best_params']\n",
    "df_results = pd.DataFrame([{k: r.get(k) for k in table_cols} for r in all_results])\n",
    "df_results = df_results.sort_values('LOO_R2', ascending=False)\n",
    "\n",
    "# Clean CSV name for display\n",
    "df_results['csv_short'] = df_results['csv'].str.replace('best_combo_','#').str.replace('.csv','')\n",
    "\n",
    "print('=' * 120)\n",
    "print('FULL RESULTS TABLE (sorted by LOO R2)')\n",
    "print('=' * 120)\n",
    "display_cols = ['csv_short', 'model', 'n_features', 'Train_R2', 'KFold_R2', 'LOO_R2', 'LOO_MAE', 'LOO_RMSE']\n",
    "print(df_results[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save\n",
    "save_path = os.path.join(STUDY_DIR, 'full_results.csv')\n",
    "df_results[display_cols + ['best_params']].to_csv(save_path, index=False)\n",
    "print(f\"\\nSaved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BEST MODEL PER CSV\n",
    "# ============================================================\n",
    "print('=' * 100)\n",
    "print('BEST MODEL PER DESCRIPTOR SET')\n",
    "print('=' * 100)\n",
    "\n",
    "best_per_csv = df_results.groupby('csv').apply(lambda g: g.nlargest(1, 'LOO_R2')).reset_index(drop=True)\n",
    "best_per_csv = best_per_csv.sort_values('LOO_R2', ascending=False)\n",
    "print(best_per_csv[display_cols].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "print('\\n')\n",
    "print('=' * 100)\n",
    "print('BEST DESCRIPTOR SET PER MODEL')\n",
    "print('=' * 100)\n",
    "\n",
    "best_per_model = df_results.groupby('model').apply(lambda g: g.nlargest(1, 'LOO_R2')).reset_index(drop=True)\n",
    "best_per_model = best_per_model.sort_values('LOO_R2', ascending=False)\n",
    "print(best_per_model[display_cols].to_string(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HEATMAP: Model x CSV -> LOO R2\n",
    "# ============================================================\n",
    "pivot = df_results.pivot_table(index='model', columns='csv_short', values='LOO_R2')\n",
    "\n",
    "# Sort by mean R2\n",
    "model_order = pivot.mean(axis=1).sort_values(ascending=False).index\n",
    "csv_order = pivot.mean(axis=0).sort_values(ascending=False).index\n",
    "pivot = pivot.loc[model_order, csv_order]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "im = ax.imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=-0.2, vmax=0.7)\n",
    "\n",
    "ax.set_xticks(range(len(csv_order)))\n",
    "ax.set_xticklabels(csv_order, rotation=45, ha='right', fontsize=8)\n",
    "ax.set_yticks(range(len(model_order)))\n",
    "ax.set_yticklabels(model_order, fontsize=9)\n",
    "\n",
    "# Annotate\n",
    "for i in range(len(model_order)):\n",
    "    for j in range(len(csv_order)):\n",
    "        val = pivot.values[i, j]\n",
    "        if not np.isnan(val):\n",
    "            color = 'white' if val < 0.1 or val > 0.55 else 'black'\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=7, color=color)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='LOO R2')\n",
    "ax.set_title('LOO R2: Model x Descriptor Set', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STUDY_DIR, 'heatmap_r2.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRED vs ACTUAL: TOP 4 OVERALL MODELS\n",
    "# ============================================================\n",
    "top4 = df_results.nlargest(4, 'LOO_R2')\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for idx, (_, row_info) in enumerate(top4.iterrows()):\n",
    "    # Find the matching result with y_pred_loo\n",
    "    match = [r for r in all_results if r['model'] == row_info['model'] and r['csv'] == row_info['csv']]\n",
    "    if not match or 'y_pred_loo' not in match[0]:\n",
    "        continue\n",
    "    \n",
    "    res = match[0]\n",
    "    df_data = pd.read_csv(os.path.join(PARENT_DIR, res['csv']))\n",
    "    y_actual = df_data['alpha_R'].values\n",
    "    y_pred = res['y_pred_loo']\n",
    "    \n",
    "    # Handle size mismatch from NaN removal\n",
    "    feat_cols = [c for c in res['features'] if c in df_data.columns]\n",
    "    X_tmp = df_data[feat_cols].values\n",
    "    valid = ~(np.isnan(X_tmp).any(axis=1) | np.isnan(y_actual))\n",
    "    y_actual = y_actual[valid]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.scatter(y_actual, y_pred, alpha=0.6, edgecolors='black', linewidth=0.5, s=35, c='steelblue')\n",
    "    lims = [min(y_actual.min(), y_pred.min()) - 0.2, max(y_actual.max(), y_pred.max()) + 0.2]\n",
    "    ax.plot(lims, lims, 'r--', lw=1.5, alpha=0.7)\n",
    "    ax.set_xlabel('Actual alpha_R (eV A)', fontsize=9)\n",
    "    ax.set_ylabel('Predicted', fontsize=9)\n",
    "    \n",
    "    csv_short = res['csv'].replace('best_combo_','#').replace('.csv','')\n",
    "    ax.set_title(f\"#{idx+1} {res['model']} | {csv_short}\\nLOO R2={res['LOO_R2']:.3f}, MAE={res['LOO_MAE']:.3f}\",\n",
    "                 fontsize=9, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "\n",
    "plt.suptitle('Top 4 Models: LOO Predicted vs Actual', fontsize=14, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STUDY_DIR, 'top4_pred_vs_actual.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BAR CHART: LOO R2 vs KFold R2 for best model per CSV\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bpc = best_per_csv.sort_values('LOO_R2', ascending=True)\n",
    "labels = bpc['csv_short'] + ' | ' + bpc['model']\n",
    "y_pos = range(len(bpc))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.barh([p - width/2 for p in y_pos], bpc['LOO_R2'], width, color='steelblue', alpha=0.8, label='LOO R2')\n",
    "bars2 = ax.barh([p + width/2 for p in y_pos], bpc['KFold_R2'], width, color='coral', alpha=0.8, label='KFold R2')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels, fontsize=8)\n",
    "ax.set_xlabel('R2 Score', fontsize=11)\n",
    "ax.set_title('Best Model per Descriptor Set: LOO vs KFold R2', fontsize=13, fontweight='bold')\n",
    "ax.axvline(x=0.45, color='gray', ls='--', lw=1, alpha=0.7, label='Previous best (0.45)')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STUDY_DIR, 'loo_vs_kfold.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESIDUAL DISTRIBUTION: BEST OVERALL MODEL\n",
    "# ============================================================\n",
    "best_overall = df_results.iloc[0]\n",
    "best_match = [r for r in all_results if r['model'] == best_overall['model'] and r['csv'] == best_overall['csv']][0]\n",
    "\n",
    "df_best = pd.read_csv(os.path.join(PARENT_DIR, best_match['csv']))\n",
    "feat_cols = [c for c in best_match['features'] if c in df_best.columns]\n",
    "X_tmp = df_best[feat_cols].values\n",
    "y_actual = df_best['alpha_R'].values\n",
    "valid = ~(np.isnan(X_tmp).any(axis=1) | np.isnan(y_actual))\n",
    "y_actual = y_actual[valid]\n",
    "y_pred = best_match['y_pred_loo']\n",
    "residuals = y_actual - y_pred\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual histogram\n",
    "ax1.hist(residuals, bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(x=0, color='red', ls='--', lw=1.5)\n",
    "ax1.set_xlabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "ax1.set_ylabel('Count', fontsize=11)\n",
    "ax1.set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.text(0.95, 0.95, f'Mean: {residuals.mean():.3f}\\nStd: {residuals.std():.3f}',\n",
    "         transform=ax1.transAxes, va='top', ha='right', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Residual vs actual\n",
    "ax2.scatter(y_actual, residuals, alpha=0.6, edgecolors='black', linewidth=0.5, s=35, c='steelblue')\n",
    "ax2.axhline(y=0, color='red', ls='--', lw=1.5)\n",
    "ax2.set_xlabel('Actual alpha_R (eV A)', fontsize=11)\n",
    "ax2.set_ylabel('Residual', fontsize=11)\n",
    "ax2.set_title('Residuals vs Actual', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "csv_short = best_match['csv'].replace('best_combo_','#').replace('.csv','')\n",
    "fig.suptitle(f\"Best Model: {best_match['model']} | {csv_short} | LOO R2={best_match['LOO_R2']:.3f}\",\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(STUDY_DIR, 'residuals_best.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "print('\\n' + '='*100)\n",
    "print('FINAL SUMMARY')\n",
    "print('='*100)\n",
    "\n",
    "top5_overall = df_results.nlargest(5, 'LOO_R2')\n",
    "print('\\nTop 5 Overall (Model + Descriptor):')\n",
    "print('-'*100)\n",
    "for i, (_, r) in enumerate(top5_overall.iterrows()):\n",
    "    csv_s = r['csv'].replace('best_combo_','#').replace('.csv','')\n",
    "    print(f\"  #{i+1}: {r['model']:<15} | {csv_s:<45} | LOO R2={r['LOO_R2']:.3f} | KF R2={r['KFold_R2']:.3f} | MAE={r['LOO_MAE']:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"Previous baseline: R2 = 0.45 (elemental features only)\")\n",
    "print(f\"Best from this study: R2 = {df_results['LOO_R2'].max():.3f}\")\n",
    "print(f\"Improvement: +{df_results['LOO_R2'].max() - 0.45:.3f}\")\n",
    "print(f\"{'='*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
