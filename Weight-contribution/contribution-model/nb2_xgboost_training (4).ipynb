{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: XGBoost Training & Comparison\n",
    "Trains XGBoost on each descriptor CSV from Notebook 1.\n",
    "Compares R2, MAE, RMSE across all scenarios.\n",
    "\n",
    "**CSV naming convention:**\n",
    "- `desc_{TYPE}_w{WINDOW}.csv` for individual types per window\n",
    "- `desc_ALL_w{WINDOW}.csv` for all types combined per window\n",
    "- `desc_ALL_multiwindow.csv` for all types across all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "BASE_DIR = r\"C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\"\n",
    "CSV_DIR = os.path.join(BASE_DIR, \"Weight-contribution\", \"contribution-model\")\n",
    "\n",
    "# Find all descriptor CSVs\n",
    "csv_files = sorted(glob.glob(os.path.join(CSV_DIR, \"desc_*.csv\")))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files:\n",
    "    name = os.path.basename(f)\n",
    "    df_tmp = pd.read_csv(f)\n",
    "    feat_cols = [c for c in df_tmp.columns if c not in \n",
    "                 ['uid','Formula','alpha_R','heavy1_el','heavy2_el','heavy1_mass',\n",
    "                  'heavy2_mass','n_elements','window']]\n",
    "    print(f\"  {name}: {len(df_tmp)} rows, {len(feat_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "# Columns to exclude from features\n",
    "NON_FEATURE_COLS = {'uid', 'Formula', 'alpha_R', 'heavy1_el', 'heavy2_el',\n",
    "                     'heavy1_mass', 'heavy2_mass', 'n_elements', 'window'}\n",
    "\n",
    "def train_and_evaluate(csv_path, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train XGBoost on a descriptor CSV.\n",
    "    Uses K-Fold CV (and LOO if n < 30).\n",
    "    Returns dict with metrics.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    \n",
    "    # Identify feature columns (numeric only, not metadata)\n",
    "    feature_cols = [c for c in df.columns if c not in NON_FEATURE_COLS]\n",
    "    # Keep only numeric\n",
    "    feature_cols = [c for c in feature_cols if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        return {'csv': csv_name, 'error': 'no features'}\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['alpha_R'].values\n",
    "    n = len(y)\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    valid = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "    X, y = X[valid], y[valid]\n",
    "    n = len(y)\n",
    "    \n",
    "    if n < 10:\n",
    "        return {'csv': csv_name, 'error': f'too few samples ({n})'}\n",
    "    \n",
    "    # XGBoost with modest params for small dataset\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'csv': csv_name,\n",
    "        'n_samples': n,\n",
    "        'n_features': len(feature_cols),\n",
    "        'features': feature_cols,\n",
    "    }\n",
    "    \n",
    "    # --- K-Fold CV ---\n",
    "    k = min(n_splits, n)\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    y_pred_kf = np.zeros(n)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        y_pred_kf[test_idx] = model.predict(X[test_idx])\n",
    "    \n",
    "    results['kfold_R2'] = r2_score(y, y_pred_kf)\n",
    "    results['kfold_MAE'] = mean_absolute_error(y, y_pred_kf)\n",
    "    results['kfold_RMSE'] = np.sqrt(mean_squared_error(y, y_pred_kf))\n",
    "    \n",
    "    # --- LOO CV (if small dataset) ---\n",
    "    if n <= 120:\n",
    "        loo = LeaveOneOut()\n",
    "        y_pred_loo = np.zeros(n)\n",
    "        for train_idx, test_idx in loo.split(X):\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            y_pred_loo[test_idx] = model.predict(X[test_idx])\n",
    "        \n",
    "        results['loo_R2'] = r2_score(y, y_pred_loo)\n",
    "        results['loo_MAE'] = mean_absolute_error(y, y_pred_loo)\n",
    "        results['loo_RMSE'] = np.sqrt(mean_squared_error(y, y_pred_loo))\n",
    "    else:\n",
    "        results['loo_R2'] = np.nan\n",
    "        results['loo_MAE'] = np.nan\n",
    "        results['loo_RMSE'] = np.nan\n",
    "    \n",
    "    # --- Feature importance (train on full data) ---\n",
    "    model.fit(X, y)\n",
    "    results['feature_importance'] = dict(zip(feature_cols, model.feature_importances_))\n",
    "    results['train_R2'] = r2_score(y, model.predict(X))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ON ALL CSVs\n",
    "# ============================================================\n",
    "all_results = []\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    name = os.path.basename(csv_path)\n",
    "    print(f\"Training on {name}...\", end=\" \")\n",
    "    res = train_and_evaluate(csv_path)\n",
    "    all_results.append(res)\n",
    "    \n",
    "    if 'error' in res:\n",
    "        print(f\"SKIPPED: {res['error']}\")\n",
    "    else:\n",
    "        loo_val = res.get('loo_R2', None)\n",
    "        loo_str = f\"{loo_val:.3f}\" if isinstance(loo_val, float) and not np.isnan(loo_val) else \"N/A\"\n",
    "        print(f\"KFold R2={res['kfold_R2']:.3f}, LOO R2={loo_str}, \"\n",
    "              f\"n={res['n_samples']}, feats={res['n_features']}\")\n",
    "\n",
    "print(\"\\nDone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ABLATION: Base p_frac + ONE elemental feature at a time\n",
    "# ============================================================\n",
    "# For each enhanced CSV, take the base p_frac columns,\n",
    "# then add one elemental feature group at a time.\n",
    "\n",
    "# Define elemental feature groups (each is a 'one addition')\n",
    "ELEM_GROUPS = {\n",
    "    'max_Z4':       ['max_Z4'],\n",
    "    'max_Z':        ['max_Z'],\n",
    "    'max_mass':     ['max_mass'],\n",
    "    'X_diff':       ['X_diff'],\n",
    "    'X_mean':       ['X_mean'],\n",
    "    'radius_diff':  ['radius_diff'],\n",
    "    'radius_mean':  ['radius_mean'],\n",
    "    'WM':           ['WM_VBM', 'WM_CBM'],\n",
    "    'WZ4':          ['WZ4_VBM', 'WZ4_CBM'],\n",
    "    'pZ4':          ['pZ4_VBM', 'pZ4_CBM'],\n",
    "    'Wr':           ['Wr_VBM', 'Wr_CBM'],\n",
    "    'WX':           ['WX_VBM', 'WX_CBM'],\n",
    "}\n",
    "\n",
    "# Find enhanced CSVs\n",
    "enhanced_csvs = [f for f in csv_files if 'enhanced' in os.path.basename(f)]\n",
    "print(f\"Enhanced CSVs found: {len(enhanced_csvs)}\")\n",
    "for f in enhanced_csvs:\n",
    "    print(f\"  {os.path.basename(f)}\")\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for csv_path in enhanced_csvs:\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Identify base p_frac columns (handle renamed ones like E_pfrac_VBM_w05)\n",
    "    base_cols = [c for c in df.columns if 'pfrac' in c.lower() or 'E_pfrac' in c]\n",
    "    base_cols = [c for c in base_cols if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "    \n",
    "    if not base_cols:\n",
    "        print(f\"  {csv_name}: No p_frac columns found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    y = df['alpha_R'].values\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{csv_name}\")\n",
    "    print(f\"Base columns: {base_cols}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # --- Baseline: just p_frac ---\n",
    "    X_base = df[base_cols].values\n",
    "    valid = ~(np.isnan(X_base).any(axis=1) | np.isnan(y))\n",
    "    X_b, y_b = X_base[valid], y[valid]\n",
    "    \n",
    "    model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                         subsample=0.8, colsample_bytree=0.8,\n",
    "                         reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=0)\n",
    "    \n",
    "    y_pred_base = np.zeros(len(y_b))\n",
    "    loo = LeaveOneOut()\n",
    "    for tr, te in loo.split(X_b):\n",
    "        model.fit(X_b[tr], y_b[tr])\n",
    "        y_pred_base[te] = model.predict(X_b[te])\n",
    "    \n",
    "    base_r2 = r2_score(y_b, y_pred_base)\n",
    "    base_mae = mean_absolute_error(y_b, y_pred_base)\n",
    "    \n",
    "    ablation_results.append({\n",
    "        'csv': csv_name,\n",
    "        'added': 'BASELINE (p_frac only)',\n",
    "        'n_features': len(base_cols),\n",
    "        'features': base_cols,\n",
    "        'LOO_R2': base_r2,\n",
    "        'LOO_MAE': base_mae,\n",
    "        'delta_R2': 0.0,\n",
    "    })\n",
    "    print(f\"  BASELINE: {len(base_cols)} feat, LOO R2={base_r2:.3f}, MAE={base_mae:.3f}\")\n",
    "    \n",
    "    # --- Add one group at a time ---\n",
    "    for grp_name, grp_cols in ELEM_GROUPS.items():\n",
    "        # Check which columns exist (handle _w05/_w10 suffixes)\n",
    "        available = []\n",
    "        for gc in grp_cols:\n",
    "            # Direct match\n",
    "            if gc in df.columns:\n",
    "                available.append(gc)\n",
    "            else:\n",
    "                # Try with window suffixes\n",
    "                for suffix in ['_w05', '_w10']:\n",
    "                    if gc + suffix in df.columns:\n",
    "                        available.append(gc + suffix)\n",
    "        \n",
    "        if not available:\n",
    "            continue\n",
    "        \n",
    "        # Check all numeric\n",
    "        available = [c for c in available if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "        if not available:\n",
    "            continue\n",
    "        \n",
    "        all_cols = base_cols + available\n",
    "        X_aug = df[all_cols].values\n",
    "        valid = ~(np.isnan(X_aug).any(axis=1) | np.isnan(y))\n",
    "        X_a, y_a = X_aug[valid], y[valid]\n",
    "        \n",
    "        y_pred_aug = np.zeros(len(y_a))\n",
    "        for tr, te in loo.split(X_a):\n",
    "            model.fit(X_a[tr], y_a[tr])\n",
    "            y_pred_aug[te] = model.predict(X_a[te])\n",
    "        \n",
    "        aug_r2 = r2_score(y_a, y_pred_aug)\n",
    "        aug_mae = mean_absolute_error(y_a, y_pred_aug)\n",
    "        delta = aug_r2 - base_r2\n",
    "        \n",
    "        marker = '+++' if delta > 0.05 else '++' if delta > 0.02 else '+' if delta > 0 else '--' if delta < -0.05 else '-' if delta < 0 else '='\n",
    "        \n",
    "        ablation_results.append({\n",
    "            'csv': csv_name,\n",
    "            'added': grp_name,\n",
    "            'n_features': len(all_cols),\n",
    "            'features': all_cols,\n",
    "            'LOO_R2': aug_r2,\n",
    "            'LOO_MAE': aug_mae,\n",
    "            'delta_R2': delta,\n",
    "        })\n",
    "        print(f\"  + {grp_name:<15} ({len(available)} col): LOO R2={aug_r2:.3f}, delta={delta:+.3f} {marker}\")\n",
    "\n",
    "print(f\"\\nTotal ablation runs: {len(ablation_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ABLATION SUMMARY: RANKED\n",
    "# ============================================================\n",
    "df_abl = pd.DataFrame(ablation_results)\n",
    "df_abl = df_abl.sort_values('LOO_R2', ascending=False)\n",
    "\n",
    "print('=' * 90)\n",
    "print('ABLATION RESULTS RANKED BY LOO R2')\n",
    "print('=' * 90)\n",
    "print(df_abl[['csv','added','n_features','LOO_R2','LOO_MAE','delta_R2']].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save\n",
    "abl_path = os.path.join(CSV_DIR, 'ablation_results.csv')\n",
    "df_abl[['csv','added','n_features','LOO_R2','LOO_MAE','delta_R2']].to_csv(abl_path, index=False)\n",
    "print(f'\\nSaved to {abl_path}')\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, max(4, len(df_abl)*0.35)))\n",
    "colors = ['green' if d > 0 else 'red' for d in df_abl['delta_R2']]\n",
    "labels = df_abl['csv'].str.replace('desc_E_enhanced_','').str.replace('.csv','') + ' + ' + df_abl['added']\n",
    "ax.barh(range(len(df_abl)), df_abl['LOO_R2'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(df_abl)))\n",
    "ax.set_yticklabels(labels, fontsize=7)\n",
    "ax.axvline(x=0.398, color='blue', ls='--', lw=1.5, label='Baseline E_w05 (0.398)')\n",
    "ax.set_xlabel('LOO R2')\n",
    "ax.set_title('Ablation: p_frac + ONE elemental feature', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CSV_DIR, 'ablation_chart.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MULTI-ELEMENTAL COMBOS: p_frac + 2 or 3 best elementals\n",
    "# ============================================================\n",
    "# From ablation: radius_mean, max_mass, Wr, max_Z/Z4 helped most.\n",
    "# Try combining them (2 at a time, 3 at a time).\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# Top elemental groups to combine\n",
    "TOP_ELEM = {\n",
    "    'radius_mean': ['radius_mean'],\n",
    "    'max_mass':    ['max_mass'],\n",
    "    'Wr':          ['Wr_VBM', 'Wr_CBM'],\n",
    "    'max_Z4':      ['max_Z4'],\n",
    "    'max_Z':       ['max_Z'],\n",
    "    'X_mean':      ['X_mean'],\n",
    "    'X_diff':      ['X_diff'],\n",
    "    'radius_diff': ['radius_diff'],\n",
    "}\n",
    "\n",
    "combo_results = []\n",
    "\n",
    "for csv_path in enhanced_csvs:\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    base_cols = [c for c in df.columns if 'pfrac' in c.lower() or 'E_pfrac' in c]\n",
    "    base_cols = [c for c in base_cols if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "    if not base_cols:\n",
    "        continue\n",
    "    \n",
    "    y = df['alpha_R'].values\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMBOS: {csv_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Try 2-element combos\n",
    "    for combo_size in [2, 3]:\n",
    "        for combo in combinations(TOP_ELEM.keys(), combo_size):\n",
    "            # Gather columns\n",
    "            extra_cols = []\n",
    "            for grp_name in combo:\n",
    "                for gc in TOP_ELEM[grp_name]:\n",
    "                    if gc in df.columns:\n",
    "                        extra_cols.append(gc)\n",
    "                    else:\n",
    "                        for suffix in ['_w05', '_w10']:\n",
    "                            if gc + suffix in df.columns:\n",
    "                                extra_cols.append(gc + suffix)\n",
    "            \n",
    "            extra_cols = [c for c in extra_cols if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "            if not extra_cols:\n",
    "                continue\n",
    "            \n",
    "            all_cols = base_cols + extra_cols\n",
    "            # Remove duplicates while preserving order\n",
    "            seen = set()\n",
    "            all_cols = [c for c in all_cols if not (c in seen or seen.add(c))]\n",
    "            \n",
    "            X = df[all_cols].values\n",
    "            valid = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "            X_v, y_v = X[valid], y[valid]\n",
    "            \n",
    "            model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                                 subsample=0.8, colsample_bytree=0.8,\n",
    "                                 reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=0)\n",
    "            \n",
    "            # LOO\n",
    "            y_pred = np.zeros(len(y_v))\n",
    "            loo = LeaveOneOut()\n",
    "            for tr, te in loo.split(X_v):\n",
    "                model.fit(X_v[tr], y_v[tr])\n",
    "                y_pred[te] = model.predict(X_v[te])\n",
    "            loo_r2 = r2_score(y_v, y_pred)\n",
    "            loo_mae = mean_absolute_error(y_v, y_pred)\n",
    "            \n",
    "            # KFold\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            y_pred_kf = np.zeros(len(y_v))\n",
    "            for tr, te in kf.split(X_v):\n",
    "                model.fit(X_v[tr], y_v[tr])\n",
    "                y_pred_kf[te] = model.predict(X_v[te])\n",
    "            kf_r2 = r2_score(y_v, y_pred_kf)\n",
    "            \n",
    "            combo_name = ' + '.join(combo)\n",
    "            combo_results.append({\n",
    "                'csv': csv_name,\n",
    "                'combo': combo_name,\n",
    "                'n_features': len(all_cols),\n",
    "                'features': all_cols,\n",
    "                'LOO_R2': loo_r2,\n",
    "                'KFold_R2': kf_r2,\n",
    "                'LOO_MAE': loo_mae,\n",
    "            })\n",
    "            \n",
    "            if loo_r2 > 0.48:\n",
    "                print(f\"  + {combo_name:<40} {len(all_cols)} feat  LOO={loo_r2:.3f}  KF={kf_r2:.3f}\")\n",
    "\n",
    "print(f\"\\nTotal combo runs: {len(combo_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMBO RESULTS: RANKED + SAVE BEST CSVs\n",
    "# ============================================================\n",
    "df_combo = pd.DataFrame(combo_results)\n",
    "df_combo = df_combo.sort_values('LOO_R2', ascending=False)\n",
    "\n",
    "print('=' * 100)\n",
    "print('TOP 20 MULTI-ELEMENTAL COMBO RESULTS')\n",
    "print('=' * 100)\n",
    "print(df_combo[['csv','combo','n_features','LOO_R2','KFold_R2','LOO_MAE']].head(20).to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save combo results\n",
    "df_combo[['csv','combo','n_features','LOO_R2','KFold_R2','LOO_MAE']].to_csv(\n",
    "    os.path.join(CSV_DIR, 'combo_results.csv'), index=False)\n",
    "\n",
    "# Save CSVs for top 5 combos that don't already have CSVs\n",
    "print(f\"\\n{'='*60}\")\n",
    "print('SAVING TOP 5 COMBO CSVs')\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for idx in range(min(5, len(df_combo))):\n",
    "    row = df_combo.iloc[idx]\n",
    "    src_csv = os.path.join(CSV_DIR, row['csv'])\n",
    "    df_src = pd.read_csv(src_csv)\n",
    "    \n",
    "    feat_cols = row['features']\n",
    "    feat_cols = [c for c in feat_cols if c in df_src.columns]\n",
    "    \n",
    "    out_cols = ['uid', 'Formula', 'alpha_R'] + feat_cols\n",
    "    combo_tag = row['combo'].replace(' + ', '_').replace(' ', '')\n",
    "    base_tag = row['csv'].replace('desc_E_enhanced_','').replace('.csv','')\n",
    "    csv_name = f\"best_combo_{idx+1}_{base_tag}_{combo_tag}.csv\"\n",
    "    csv_path = os.path.join(CSV_DIR, csv_name)\n",
    "    df_src[out_cols].to_csv(csv_path, index=False)\n",
    "    print(f\"  #{idx+1}: {csv_name}\")\n",
    "    print(f\"       LOO R2={row['LOO_R2']:.3f}, KFold R2={row['KFold_R2']:.3f}, {len(feat_cols)} features\")\n",
    "    print(f\"       Features: {feat_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL: TOP 5 OVERALL (ablation + combos + gradient)\n",
    "# ============================================================\n",
    "# Merge all results\n",
    "all_ranked = []\n",
    "\n",
    "# From ablation\n",
    "for r in ablation_results:\n",
    "    all_ranked.append({\n",
    "        'source': 'ablation',\n",
    "        'description': r['csv'].replace('desc_E_enhanced_','').replace('.csv','') + ' + ' + r['added'],\n",
    "        'n_features': r['n_features'],\n",
    "        'features': r['features'],\n",
    "        'LOO_R2': r['LOO_R2'],\n",
    "        'LOO_MAE': r['LOO_MAE'],\n",
    "        'csv_source': r['csv'],\n",
    "    })\n",
    "\n",
    "# From combos\n",
    "for _, r in df_combo.iterrows():\n",
    "    all_ranked.append({\n",
    "        'source': 'combo',\n",
    "        'description': r['csv'].replace('desc_E_enhanced_','').replace('.csv','') + ' + ' + r['combo'],\n",
    "        'n_features': r['n_features'],\n",
    "        'features': r['features'],\n",
    "        'LOO_R2': r['LOO_R2'],\n",
    "        'LOO_MAE': r['LOO_MAE'],\n",
    "        'csv_source': r['csv'],\n",
    "    })\n",
    "\n",
    "# From original scenarios (non-ablation)\n",
    "for r in all_results:\n",
    "    if 'error' in r:\n",
    "        continue\n",
    "    all_ranked.append({\n",
    "        'source': 'original',\n",
    "        'description': r['csv'].replace('.csv',''),\n",
    "        'n_features': r['n_features'],\n",
    "        'features': r.get('features', []),\n",
    "        'LOO_R2': r.get('loo_R2', np.nan),\n",
    "        'LOO_MAE': r.get('loo_MAE', np.nan),\n",
    "        'csv_source': r['csv'],\n",
    "    })\n",
    "\n",
    "df_all = pd.DataFrame(all_ranked)\n",
    "df_all = df_all.sort_values('LOO_R2', ascending=False)\n",
    "\n",
    "print('=' * 100)\n",
    "print('\\U0001f3c6 TOP 10 OVERALL MODELS \\U0001f3c6')\n",
    "print('=' * 100)\n",
    "for idx in range(min(10, len(df_all))):\n",
    "    r = df_all.iloc[idx]\n",
    "    print(f\"\\n#{idx+1}: {r['description']}\")\n",
    "    print(f\"    Source: {r['source']}, Features: {r['n_features']}, LOO R2: {r['LOO_R2']:.3f}, MAE: {r['LOO_MAE']:.3f}\")\n",
    "    if isinstance(r['features'], list):\n",
    "        print(f\"    Cols: {r['features']}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print('RECOMMENDATION: Take top 5 and run detailed model comparison (LinReg, RF, XGB, TabNet, etc.)')\n",
    "print('CSVs for top 5 combos saved as best_combo_*.csv')\n",
    "print(f\"{'='*100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "summary_rows = []\n",
    "for res in all_results:\n",
    "    if 'error' in res:\n",
    "        continue\n",
    "    summary_rows.append({\n",
    "        'CSV': res['csv'],\n",
    "        'n': res['n_samples'],\n",
    "        'features': res['n_features'],\n",
    "        'train_R2': res['train_R2'],\n",
    "        'KFold_R2': res['kfold_R2'],\n",
    "        'KFold_MAE': res['kfold_MAE'],\n",
    "        'KFold_RMSE': res['kfold_RMSE'],\n",
    "        'LOO_R2': res.get('loo_R2', np.nan),\n",
    "        'LOO_MAE': res.get('loo_MAE', np.nan),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_summary = df_summary.sort_values('KFold_R2', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"RESULTS RANKED BY KFold R2\")\n",
    "print(\"=\" * 100)\n",
    "print(df_summary.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save\n",
    "summary_path = os.path.join(CSV_DIR, \"results_summary.csv\")\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSaved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: R2 COMPARISON BAR CHART\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "df_plot = df_summary.sort_values('KFold_R2', ascending=True)\n",
    "y_pos = range(len(df_plot))\n",
    "\n",
    "bars = ax.barh(y_pos, df_plot['KFold_R2'], color='steelblue', alpha=0.8, label='KFold R2')\n",
    "if 'LOO_R2' in df_plot.columns:\n",
    "    ax.barh(y_pos, df_plot['LOO_R2'], color='coral', alpha=0.5, height=0.4, label='LOO R2')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df_plot['CSV'].str.replace('desc_', '').str.replace('.csv', ''), fontsize=8)\n",
    "ax.set_xlabel('R2 Score', fontsize=12)\n",
    "ax.set_title('XGBoost R2 by Descriptor Scenario', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0.45, color='red', ls='--', lw=1, alpha=0.7, label='Previous best (0.45)')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CSV_DIR, 'r2_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE FOR TOP 3 ABLATION MODELS\n",
    "# ============================================================\n",
    "df_abl_sorted = pd.DataFrame(ablation_results).sort_values('LOO_R2', ascending=False)\n",
    "\n",
    "for idx in range(min(3, len(df_abl_sorted))):\n",
    "    row = df_abl_sorted.iloc[idx]\n",
    "    csv_path = os.path.join(CSV_DIR, row['csv'])\n",
    "    feat_cols = row['features']\n",
    "    \n",
    "    df_data = pd.read_csv(csv_path)\n",
    "    feat_cols = [c for c in feat_cols if c in df_data.columns]\n",
    "    \n",
    "    X = df_data[feat_cols].values\n",
    "    y = df_data['alpha_R'].values\n",
    "    valid = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "    X, y = X[valid], y[valid]\n",
    "    \n",
    "    model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                         subsample=0.8, colsample_bytree=0.8,\n",
    "                         reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=0)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    label = row['csv'].replace('desc_E_enhanced_','').replace('.csv','') + ' + ' + row['added']\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model #{idx+1}: {label} (LOO R2={row['LOO_R2']:.3f})\")\n",
    "    print(f\"{'='*50}\")\n",
    "    fi = dict(zip(feat_cols, model.feature_importances_))\n",
    "    for feat, imp in sorted(fi.items(), key=lambda x: x[1], reverse=True):\n",
    "        bar = '|' * int(imp * 40)\n",
    "        print(f\"  {feat:<25} {imp:.3f} {bar}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRED vs ACTUAL SCATTER FOR TOP 5 ABLATION MODELS\n",
    "# ============================================================\n",
    "# Sort ablation results by LOO_R2\n",
    "df_abl_sorted = pd.DataFrame(ablation_results).sort_values('LOO_R2', ascending=False)\n",
    "top_n = min(5, len(df_abl_sorted))\n",
    "\n",
    "fig, axes = plt.subplots(1, top_n, figsize=(5*top_n, 5))\n",
    "if top_n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx in range(top_n):\n",
    "    row = df_abl_sorted.iloc[idx]\n",
    "    csv_path = os.path.join(CSV_DIR, row['csv'])\n",
    "    feat_cols = row['features']\n",
    "    \n",
    "    df_data = pd.read_csv(csv_path)\n",
    "    # Check all feature cols exist\n",
    "    feat_cols = [c for c in feat_cols if c in df_data.columns]\n",
    "    \n",
    "    X = df_data[feat_cols].values\n",
    "    y = df_data['alpha_R'].values\n",
    "    valid = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "    X, y = X[valid], y[valid]\n",
    "    \n",
    "    model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                         subsample=0.8, colsample_bytree=0.8,\n",
    "                         reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=0)\n",
    "    \n",
    "    y_pred = np.zeros(len(y))\n",
    "    loo = LeaveOneOut()\n",
    "    for tr, te in loo.split(X):\n",
    "        model.fit(X[tr], y[tr])\n",
    "        y_pred[te] = model.predict(X[te])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.scatter(y, y_pred, alpha=0.6, edgecolors='black', linewidth=0.5, s=30)\n",
    "    lims = [min(y.min(), y_pred.min()) - 0.2, max(y.max(), y_pred.max()) + 0.2]\n",
    "    ax.plot(lims, lims, 'r--', lw=1.5)\n",
    "    ax.set_xlabel('Actual', fontsize=10)\n",
    "    ax.set_ylabel('Predicted', fontsize=10)\n",
    "    \n",
    "    label = row['csv'].replace('desc_E_enhanced_','').replace('.csv','') + ' + ' + row['added']\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    ax.set_title(f'#{idx+1}: {label}\\nR2={r2:.3f}, MAE={mae:.3f}', fontsize=9, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "\n",
    "plt.suptitle('Top 5 Ablation Models: LOO Predicted vs Actual', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CSV_DIR, 'top5_pred_vs_actual.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}