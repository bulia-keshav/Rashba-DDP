{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: XGBoost Training & Comparison\n",
    "Trains XGBoost on each descriptor CSV from Notebook 1.\n",
    "Compares R2, MAE, RMSE across all scenarios.\n",
    "\n",
    "**CSV naming convention:**\n",
    "- `desc_{TYPE}_w{WINDOW}.csv` for individual types per window\n",
    "- `desc_ALL_w{WINDOW}.csv` for all types combined per window\n",
    "- `desc_ALL_multiwindow.csv` for all types across all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "BASE_DIR = r\"C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\"\n",
    "CSV_DIR = os.path.join(BASE_DIR, \"Weight-contribution\", \"contribution-model\")\n",
    "\n",
    "# Find all descriptor CSVs\n",
    "csv_files = sorted(glob.glob(os.path.join(CSV_DIR, \"desc_*.csv\")))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files:\n",
    "    name = os.path.basename(f)\n",
    "    df_tmp = pd.read_csv(f)\n",
    "    feat_cols = [c for c in df_tmp.columns if c not in \n",
    "                 ['uid','Formula','alpha_R','heavy1_el','heavy2_el','heavy1_mass',\n",
    "                  'heavy2_mass','n_elements','window']]\n",
    "    print(f\"  {name}: {len(df_tmp)} rows, {len(feat_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "# Columns to exclude from features\n",
    "NON_FEATURE_COLS = {'uid', 'Formula', 'alpha_R', 'heavy1_el', 'heavy2_el',\n",
    "                     'heavy1_mass', 'heavy2_mass', 'n_elements', 'window'}\n",
    "\n",
    "def train_and_evaluate(csv_path, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train XGBoost on a descriptor CSV.\n",
    "    Uses K-Fold CV (and LOO if n < 30).\n",
    "    Returns dict with metrics.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    \n",
    "    # Identify feature columns (numeric only, not metadata)\n",
    "    feature_cols = [c for c in df.columns if c not in NON_FEATURE_COLS]\n",
    "    # Keep only numeric\n",
    "    feature_cols = [c for c in feature_cols if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        return {'csv': csv_name, 'error': 'no features'}\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['alpha_R'].values\n",
    "    n = len(y)\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    valid = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "    X, y = X[valid], y[valid]\n",
    "    n = len(y)\n",
    "    \n",
    "    if n < 10:\n",
    "        return {'csv': csv_name, 'error': f'too few samples ({n})'}\n",
    "    \n",
    "    # XGBoost with modest params for small dataset\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'csv': csv_name,\n",
    "        'n_samples': n,\n",
    "        'n_features': len(feature_cols),\n",
    "        'features': feature_cols,\n",
    "    }\n",
    "    \n",
    "    # --- K-Fold CV ---\n",
    "    k = min(n_splits, n)\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    y_pred_kf = np.zeros(n)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        y_pred_kf[test_idx] = model.predict(X[test_idx])\n",
    "    \n",
    "    results['kfold_R2'] = r2_score(y, y_pred_kf)\n",
    "    results['kfold_MAE'] = mean_absolute_error(y, y_pred_kf)\n",
    "    results['kfold_RMSE'] = np.sqrt(mean_squared_error(y, y_pred_kf))\n",
    "    \n",
    "    # --- LOO CV (if small dataset) ---\n",
    "    if n <= 120:\n",
    "        loo = LeaveOneOut()\n",
    "        y_pred_loo = np.zeros(n)\n",
    "        for train_idx, test_idx in loo.split(X):\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            y_pred_loo[test_idx] = model.predict(X[test_idx])\n",
    "        \n",
    "        results['loo_R2'] = r2_score(y, y_pred_loo)\n",
    "        results['loo_MAE'] = mean_absolute_error(y, y_pred_loo)\n",
    "        results['loo_RMSE'] = np.sqrt(mean_squared_error(y, y_pred_loo))\n",
    "    else:\n",
    "        results['loo_R2'] = np.nan\n",
    "        results['loo_MAE'] = np.nan\n",
    "        results['loo_RMSE'] = np.nan\n",
    "    \n",
    "    # --- Feature importance (train on full data) ---\n",
    "    model.fit(X, y)\n",
    "    results['feature_importance'] = dict(zip(feature_cols, model.feature_importances_))\n",
    "    results['train_R2'] = r2_score(y, model.predict(X))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ON ALL CSVs\n",
    "# ============================================================\n",
    "all_results = []\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    name = os.path.basename(csv_path)\n",
    "    print(f\"Training on {name}...\", end=\" \")\n",
    "    res = train_and_evaluate(csv_path)\n",
    "    all_results.append(res)\n",
    "    \n",
    "    if 'error' in res:\n",
    "        print(f\"SKIPPED: {res['error']}\")\n",
    "    else:\n",
    "        loo_val = res.get('loo_R2', None)\n",
    "        loo_str = f\"{loo_val:.3f}\" if isinstance(loo_val, float) and not np.isnan(loo_val) else \"N/A\"\n",
    "        print(f\"KFold R2={res['kfold_R2']:.3f}, LOO R2={loo_str}, \"\n",
    "              f\"n={res['n_samples']}, feats={res['n_features']}\")\n",
    "\n",
    "print(\"\\nDone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ABLATION: Base p_frac + ONE elemental feature at a time\n",
    "# ============================================================\n",
    "# For each enhanced CSV, take the base p_frac columns,\n",
    "# then add one elemental feature group at a time.\n",
    "\n",
    "# Define elemental feature groups (each is a 'one addition')\n",
    "ELEM_GROUPS = {\n",
    "    'max_Z4':       ['max_Z4'],\n",
    "    'max_Z':        ['max_Z'],\n",
    "    'max_mass':     ['max_mass'],\n",
    "    'X_diff':       ['X_diff'],\n",
    "    'X_mean':       ['X_mean'],\n",
    "    'radius_diff':  ['radius_diff'],\n",
    "    'radius_mean':  ['radius_mean'],\n",
    "    'WM':           ['WM_VBM', 'WM_CBM'],\n",
    "    'WZ4':          ['WZ4_VBM', 'WZ4_CBM'],\n",
    "    'pZ4':          ['pZ4_VBM', 'pZ4_CBM'],\n",
    "    'Wr':           ['Wr_VBM', 'Wr_CBM'],\n",
    "    'WX':           ['WX_VBM', 'WX_CBM'],\n",
    "}\n",
    "\n",
    "# Find enhanced CSVs\n",
    "enhanced_csvs = [f for f in csv_files if 'enhanced' in os.path.basename(f)]\n",
    "print(f\"Enhanced CSVs found: {len(enhanced_csvs)}\")\n",
    "for f in enhanced_csvs:\n",
    "    print(f\"  {os.path.basename(f)}\")\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "for csv_path in enhanced_csvs:\n",
    "    csv_name = os.path.basename(csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Identify base p_frac columns (handle renamed ones like E_pfrac_VBM_w05)\n",
    "    base_cols = [c for c in df.columns if 'pfrac' in c.lower() or 'E_pfrac' in c]\n",
    "    base_cols = [c for c in base_cols if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "    \n",
    "    if not base_cols:\n",
    "        print(f\"  {csv_name}: No p_frac columns found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    y = df['alpha_R'].values\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{csv_name}\")\n",
    "    print(f\"Base columns: {base_cols}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # --- Baseline: just p_frac ---\n",
    "    X_base = df[base_cols].values\n",
    "    valid = ~(np.isnan(X_base).any(axis=1) | np.isnan(y))\n",
    "    X_b, y_b = X_base[valid], y[valid]\n",
    "    \n",
    "    model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                         subsample=0.8, colsample_bytree=0.8,\n",
    "                         reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=0)\n",
    "    \n",
    "    y_pred_base = np.zeros(len(y_b))\n",
    "    loo = LeaveOneOut()\n",
    "    for tr, te in loo.split(X_b):\n",
    "        model.fit(X_b[tr], y_b[tr])\n",
    "        y_pred_base[te] = model.predict(X_b[te])\n",
    "    \n",
    "    base_r2 = r2_score(y_b, y_pred_base)\n",
    "    base_mae = mean_absolute_error(y_b, y_pred_base)\n",
    "    \n",
    "    ablation_results.append({\n",
    "        'csv': csv_name,\n",
    "        'added': 'BASELINE (p_frac only)',\n",
    "        'n_features': len(base_cols),\n",
    "        'features': base_cols,\n",
    "        'LOO_R2': base_r2,\n",
    "        'LOO_MAE': base_mae,\n",
    "        'delta_R2': 0.0,\n",
    "    })\n",
    "    print(f\"  BASELINE: {len(base_cols)} feat, LOO R2={base_r2:.3f}, MAE={base_mae:.3f}\")\n",
    "    \n",
    "    # --- Add one group at a time ---\n",
    "    for grp_name, grp_cols in ELEM_GROUPS.items():\n",
    "        # Check which columns exist (handle _w05/_w10 suffixes)\n",
    "        available = []\n",
    "        for gc in grp_cols:\n",
    "            # Direct match\n",
    "            if gc in df.columns:\n",
    "                available.append(gc)\n",
    "            else:\n",
    "                # Try with window suffixes\n",
    "                for suffix in ['_w05', '_w10']:\n",
    "                    if gc + suffix in df.columns:\n",
    "                        available.append(gc + suffix)\n",
    "        \n",
    "        if not available:\n",
    "            continue\n",
    "        \n",
    "        # Check all numeric\n",
    "        available = [c for c in available if df[c].dtype in ['float64','float32','int64','int32']]\n",
    "        if not available:\n",
    "            continue\n",
    "        \n",
    "        all_cols = base_cols + available\n",
    "        X_aug = df[all_cols].values\n",
    "        valid = ~(np.isnan(X_aug).any(axis=1) | np.isnan(y))\n",
    "        X_a, y_a = X_aug[valid], y[valid]\n",
    "        \n",
    "        y_pred_aug = np.zeros(len(y_a))\n",
    "        for tr, te in loo.split(X_a):\n",
    "            model.fit(X_a[tr], y_a[tr])\n",
    "            y_pred_aug[te] = model.predict(X_a[te])\n",
    "        \n",
    "        aug_r2 = r2_score(y_a, y_pred_aug)\n",
    "        aug_mae = mean_absolute_error(y_a, y_pred_aug)\n",
    "        delta = aug_r2 - base_r2\n",
    "        \n",
    "        marker = '+++' if delta > 0.05 else '++' if delta > 0.02 else '+' if delta > 0 else '--' if delta < -0.05 else '-' if delta < 0 else '='\n",
    "        \n",
    "        ablation_results.append({\n",
    "            'csv': csv_name,\n",
    "            'added': grp_name,\n",
    "            'n_features': len(all_cols),\n",
    "            'features': all_cols,\n",
    "            'LOO_R2': aug_r2,\n",
    "            'LOO_MAE': aug_mae,\n",
    "            'delta_R2': delta,\n",
    "        })\n",
    "        print(f\"  + {grp_name:<15} ({len(available)} col): LOO R2={aug_r2:.3f}, delta={delta:+.3f} {marker}\")\n",
    "\n",
    "print(f\"\\nTotal ablation runs: {len(ablation_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ABLATION SUMMARY: RANKED\n",
    "# ============================================================\n",
    "df_abl = pd.DataFrame(ablation_results)\n",
    "df_abl = df_abl.sort_values('LOO_R2', ascending=False)\n",
    "\n",
    "print('=' * 90)\n",
    "print('ABLATION RESULTS RANKED BY LOO R2')\n",
    "print('=' * 90)\n",
    "print(df_abl[['csv','added','n_features','LOO_R2','LOO_MAE','delta_R2']].to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save\n",
    "abl_path = os.path.join(CSV_DIR, 'ablation_results.csv')\n",
    "df_abl[['csv','added','n_features','LOO_R2','LOO_MAE','delta_R2']].to_csv(abl_path, index=False)\n",
    "print(f'\\nSaved to {abl_path}')\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, max(4, len(df_abl)*0.35)))\n",
    "colors = ['green' if d > 0 else 'red' for d in df_abl['delta_R2']]\n",
    "labels = df_abl['csv'].str.replace('desc_E_enhanced_','').str.replace('.csv','') + ' + ' + df_abl['added']\n",
    "ax.barh(range(len(df_abl)), df_abl['LOO_R2'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(df_abl)))\n",
    "ax.set_yticklabels(labels, fontsize=7)\n",
    "ax.axvline(x=0.398, color='blue', ls='--', lw=1.5, label='Baseline E_w05 (0.398)')\n",
    "ax.set_xlabel('LOO R2')\n",
    "ax.set_title('Ablation: p_frac + ONE elemental feature', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CSV_DIR, 'ablation_chart.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "summary_rows = []\n",
    "for res in all_results:\n",
    "    if 'error' in res:\n",
    "        continue\n",
    "    summary_rows.append({\n",
    "        'CSV': res['csv'],\n",
    "        'n': res['n_samples'],\n",
    "        'features': res['n_features'],\n",
    "        'train_R2': res['train_R2'],\n",
    "        'KFold_R2': res['kfold_R2'],\n",
    "        'KFold_MAE': res['kfold_MAE'],\n",
    "        'KFold_RMSE': res['kfold_RMSE'],\n",
    "        'LOO_R2': res.get('loo_R2', np.nan),\n",
    "        'LOO_MAE': res.get('loo_MAE', np.nan),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_summary = df_summary.sort_values('KFold_R2', ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"RESULTS RANKED BY KFold R2\")\n",
    "print(\"=\" * 100)\n",
    "print(df_summary.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save\n",
    "summary_path = os.path.join(CSV_DIR, \"results_summary.csv\")\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSaved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: R2 COMPARISON BAR CHART\n",
    "# ============================================================\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "df_plot = df_summary.sort_values('KFold_R2', ascending=True)\n",
    "y_pos = range(len(df_plot))\n",
    "\n",
    "bars = ax.barh(y_pos, df_plot['KFold_R2'], color='steelblue', alpha=0.8, label='KFold R2')\n",
    "if 'LOO_R2' in df_plot.columns:\n",
    "    ax.barh(y_pos, df_plot['LOO_R2'], color='coral', alpha=0.5, height=0.4, label='LOO R2')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df_plot['CSV'].str.replace('desc_', '').str.replace('.csv', ''), fontsize=8)\n",
    "ax.set_xlabel('R2 Score', fontsize=12)\n",
    "ax.set_title('XGBoost R2 by Descriptor Scenario', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0.45, color='red', ls='--', lw=1, alpha=0.7, label='Previous best (0.45)')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CSV_DIR, 'r2_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE FOR BEST MODEL\n",
    "# ============================================================\n",
    "best_csv = df_summary.iloc[0]['CSV']\n",
    "best_res = [r for r in all_results if r.get('csv') == best_csv][0]\n",
    "\n",
    "print(f\"Best model: {best_csv}\")\n",
    "print(f\"KFold R2: {best_res['kfold_R2']:.3f}\")\n",
    "print(f\"LOO R2: {best_res.get('loo_R2', 'N/A')}\")\n",
    "print(f\"\\nFeature Importance:\")\n",
    "\n",
    "fi = best_res['feature_importance']\n",
    "fi_sorted = sorted(fi.items(), key=lambda x: x[1], reverse=True)\n",
    "for feat, imp in fi_sorted:\n",
    "    bar = '|' * int(imp * 50)\n",
    "    print(f\"  {feat:<25} {imp:.3f} {bar}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, max(4, len(fi_sorted)*0.4)))\n",
    "feats = [f[0] for f in fi_sorted]\n",
    "imps = [f[1] for f in fi_sorted]\n",
    "ax.barh(range(len(feats)), imps, color='steelblue')\n",
    "ax.set_yticks(range(len(feats)))\n",
    "ax.set_yticklabels(feats, fontsize=9)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title(f'Feature Importance: {best_csv}', fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CSV_DIR, 'best_feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRED vs ACTUAL SCATTER FOR BEST MODEL\n",
    "# ============================================================\n",
    "best_path = os.path.join(CSV_DIR, best_csv)\n",
    "df_best = pd.read_csv(best_path)\n",
    "feat_cols = [c for c in df_best.columns if c not in NON_FEATURE_COLS \n",
    "             and df_best[c].dtype in ['float64','float32','int64','int32']]\n",
    "\n",
    "X = df_best[feat_cols].values\n",
    "y = df_best['alpha_R'].values\n",
    "valid = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n",
    "X, y = X[valid], y[valid]\n",
    "\n",
    "# LOO predictions\n",
    "model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1,\n",
    "                     subsample=0.8, colsample_bytree=0.8,\n",
    "                     reg_alpha=1.0, reg_lambda=1.0, random_state=42, verbosity=0)\n",
    "\n",
    "y_pred = np.zeros(len(y))\n",
    "loo = LeaveOneOut()\n",
    "for train_idx, test_idx in loo.split(X):\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    y_pred[test_idx] = model.predict(X[test_idx])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(y, y_pred, alpha=0.6, edgecolors='black', linewidth=0.5, s=40)\n",
    "lims = [min(y.min(), y_pred.min()) - 0.2, max(y.max(), y_pred.max()) + 0.2]\n",
    "ax.plot(lims, lims, 'r--', lw=1.5, label='Perfect prediction')\n",
    "ax.set_xlabel('Actual alpha_R (eV A)', fontsize=12)\n",
    "ax.set_ylabel('Predicted alpha_R (eV A)', fontsize=12)\n",
    "ax.set_title(f'LOO: {best_csv}\\nR2={r2_score(y,y_pred):.3f}, MAE={mean_absolute_error(y,y_pred):.3f}',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CSV_DIR, 'best_pred_vs_actual.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}