{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Descriptor Extraction\n",
    "Extracts orbital-weight-based descriptors from `vasprun.xml` for all Rashba compounds.\n",
    "\n",
    "**Descriptor Types:**\n",
    "- **A** `WM_total`: sum(w_X * M_X) at VBM/CBM\n",
    "- **B** `WM_p_only`: sum(p_X * M_X) using only p-orbital contributions\n",
    "- **C** `WM_p_frac`: p_i*M_i / sum(p_j*M_j) for top 2 heaviest elements\n",
    "- **D** `WM_indiv`: w_heavy1 * M_heavy1, w_heavy2 * M_heavy2 individually\n",
    "- **E** `p_frac`: total p-orbital fraction\n",
    "- **F** `p_heavy`: p_frac_of_heaviest * M_heaviest for top 2\n",
    "\n",
    "**Windows:** 0.05, 0.1, 0.5 eV\n",
    "\n",
    "**Target:** max(Rashba_parameter) per UID from rashba.csv\n",
    "\n",
    "**Output CSVs:** one per (type, window) combo + combined CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pymatgen.io.vasp.outputs import Vasprun\n",
    "from pymatgen.electronic_structure.core import Spin, OrbitalType\n",
    "from pymatgen.core.periodic_table import Element\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rashba CSV: C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\\Data\\rashba.csv\n",
      "Rashba compounds dir: C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\\Inverse-design\\rashba\n",
      "Output dir: C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\\Weight-contribution\\contribution-model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PATHS - ADJUST THESE\n",
    "# ============================================================\n",
    "BASE_DIR = r\"C:\\Users\\AbCMS_Lab\\Desktop\\Keshav-DDP\"\n",
    "RASHBA_CSV = os.path.join(BASE_DIR, \"Data\", \"rashba.csv\")\n",
    "RASHBA_DIR = os.path.join(BASE_DIR, \"Inverse-design\", \"rashba\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"Weight-contribution\", \"contribution-model\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Windows\n",
    "WINDOWS = [0.05, 0.1, 0.5]\n",
    "\n",
    "print(f\"Rashba CSV: {RASHBA_CSV}\")\n",
    "print(f\"Rashba compounds dir: {RASHBA_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in rashba.csv: 205\n",
      "Unique UIDs: 99\n",
      "Columns: ['Formula', 'uid', 'spacegroup', 'ehull', 'bandgap', 'band', 'kpath', 'Rashba_parameter', 'SS', 'dE', 'anticrossing']\n",
      "\n",
      "Target: 99 compounds with max alpha_R\n",
      "            uid  alpha_R_max  Formula\n",
      "0  001e03f2c095        3.288     SSeW\n",
      "1  03bcf7dcdaf2        4.804   Sn2Te2\n",
      "2  04fdd7d1ec5c        1.018   ClSbTe\n",
      "3  05a06afa3b20        1.643  WMo3Se8\n",
      "4  0b7696e1f4c9        1.756  CrW3Se8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD RASHBA CSV & GET TARGET (max alpha_R per UID)\n",
    "# ============================================================\n",
    "df_rashba = pd.read_csv(RASHBA_CSV)\n",
    "print(f\"Total rows in rashba.csv: {len(df_rashba)}\")\n",
    "print(f\"Unique UIDs: {df_rashba['uid'].nunique()}\")\n",
    "print(f\"Columns: {list(df_rashba.columns)}\")\n",
    "\n",
    "# Max Rashba parameter per UID\n",
    "target = df_rashba.groupby('uid')['Rashba_parameter'].max().reset_index()\n",
    "target.columns = ['uid', 'alpha_R_max']\n",
    "\n",
    "# Also keep formula for reference\n",
    "uid_formula = df_rashba.groupby('uid')['Formula'].first().reset_index()\n",
    "target = target.merge(uid_formula, on='uid')\n",
    "\n",
    "print(f\"\\nTarget: {len(target)} compounds with max alpha_R\")\n",
    "print(target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 compound directories with vasprun.xml\n",
      "Matched: 0/99\n",
      "Unmatched UIDs (first 10): ['001e03f2c095', '03bcf7dcdaf2', '04fdd7d1ec5c', '05a06afa3b20', '0b7696e1f4c9', '0c0fbdaf8f4a', '0f02957b17cf', '114b3382699c', '11db0908d9ef', '159f028a85d0']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FIND VASPRUN FILES FOR EACH UID\n",
    "# ============================================================\n",
    "compound_dirs = {}\n",
    "for folder in os.listdir(RASHBA_DIR):\n",
    "    folder_path = os.path.join(RASHBA_DIR, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    # Extract UID from folder name (e.g., ISbSe-343d2125478e -> 343d2125478e)\n",
    "    parts = folder.split('-')\n",
    "    if len(parts) >= 2:\n",
    "        uid_candidate = parts[-1]\n",
    "        # Find vasprun.xml\n",
    "        vasprun_pattern = os.path.join(folder_path, \"**\", \"vasprun.xml\")\n",
    "        vasprun_files = glob.glob(vasprun_pattern, recursive=True)\n",
    "        if vasprun_files:\n",
    "            compound_dirs[uid_candidate] = {\n",
    "                'folder': folder,\n",
    "                'vasprun': vasprun_files[0]\n",
    "            }\n",
    "\n",
    "print(f\"Found {len(compound_dirs)} compound directories with vasprun.xml\")\n",
    "\n",
    "# Match with target UIDs\n",
    "# UIDs in CSV might be partial matches with folder names\n",
    "matched = 0\n",
    "unmatched_uids = []\n",
    "for _, row in target.iterrows():\n",
    "    uid = row['uid']\n",
    "    # Try direct match first\n",
    "    if uid in compound_dirs:\n",
    "        matched += 1\n",
    "    else:\n",
    "        # Try partial match (uid might be substring of folder uid)\n",
    "        found = False\n",
    "        for folder_uid in compound_dirs:\n",
    "            if uid.startswith(folder_uid) or folder_uid.startswith(uid):\n",
    "                compound_dirs[uid] = compound_dirs[folder_uid]\n",
    "                matched += 1\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            unmatched_uids.append(uid)\n",
    "\n",
    "print(f\"Matched: {matched}/{len(target)}\")\n",
    "if unmatched_uids:\n",
    "    print(f\"Unmatched UIDs (first 10): {unmatched_uids[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORE FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def get_dos_array(dos_obj):\n",
    "    \"\"\"Extract DOS values, summing spin channels for SOC.\"\"\"\n",
    "    if Spin.down in dos_obj.densities:\n",
    "        return dos_obj.densities[Spin.up] + dos_obj.densities[Spin.down]\n",
    "    return dos_obj.densities[Spin.up]\n",
    "\n",
    "def integrate_window(energies, dos_vals, window):\n",
    "    \"\"\"Integrate DOS in energy window using trapezoidal rule.\"\"\"\n",
    "    mask = (energies >= window[0]) & (energies <= window[1])\n",
    "    e_w = energies[mask]\n",
    "    d_w = dos_vals[mask]\n",
    "    if len(e_w) < 2:\n",
    "        return 0.0\n",
    "    return np.trapezoid(d_w, e_w)\n",
    "\n",
    "def extract_contributions(vasprun_path, window_size):\n",
    "    \"\"\"\n",
    "    Parse vasprun.xml and extract orbital/atomic contributions at VBM/CBM.\n",
    "    Returns dict with per-element per-orbital contributions (normalized).\n",
    "    \"\"\"\n",
    "    vr = Vasprun(vasprun_path, parse_dos=True, parse_eigen=False)\n",
    "    cdos = vr.complete_dos\n",
    "    e_fermi = vr.efermi\n",
    "    \n",
    "    vbm = cdos.get_cbm_vbm()[1]\n",
    "    cbm = cdos.get_cbm_vbm()[0]\n",
    "    \n",
    "    vbm_s = vbm - e_fermi\n",
    "    cbm_s = cbm - e_fermi\n",
    "    energies = cdos.energies - e_fermi\n",
    "    \n",
    "    vbm_win = [vbm_s - window_size, vbm_s]\n",
    "    cbm_win = [cbm_s, cbm_s + window_size]\n",
    "    \n",
    "    element_dos = cdos.get_element_dos()\n",
    "    orbital_map = {'s': OrbitalType.s, 'p': OrbitalType.p, 'd': OrbitalType.d}\n",
    "    \n",
    "    # Raw contributions\n",
    "    raw = {}\n",
    "    for element in element_dos:\n",
    "        el = str(element)\n",
    "        mass = Element(el).atomic_mass\n",
    "        Z = Element(el).Z\n",
    "        raw[el] = {'mass': float(mass), 'Z': Z}\n",
    "        \n",
    "        spd = cdos.get_element_spd_dos(element)\n",
    "        for orb_str in ['s', 'p', 'd']:\n",
    "            orb_type = orbital_map[orb_str]\n",
    "            if orb_type in spd:\n",
    "                dos_vals = get_dos_array(spd[orb_type])\n",
    "                raw[el][f'{orb_str}_VBM'] = integrate_window(energies, dos_vals, vbm_win)\n",
    "                raw[el][f'{orb_str}_CBM'] = integrate_window(energies, dos_vals, cbm_win)\n",
    "            else:\n",
    "                raw[el][f'{orb_str}_VBM'] = 0.0\n",
    "                raw[el][f'{orb_str}_CBM'] = 0.0\n",
    "    \n",
    "    # Total per band edge\n",
    "    total_vbm = sum(raw[el][f'{o}_VBM'] for el in raw for o in ['s','p','d'])\n",
    "    total_cbm = sum(raw[el][f'{o}_CBM'] for el in raw for o in ['s','p','d'])\n",
    "    \n",
    "    # Normalized\n",
    "    for el in raw:\n",
    "        for o in ['s','p','d']:\n",
    "            raw[el][f'{o}_VBM_norm'] = raw[el][f'{o}_VBM'] / total_vbm if total_vbm > 0 else 0\n",
    "            raw[el][f'{o}_CBM_norm'] = raw[el][f'{o}_CBM'] / total_cbm if total_cbm > 0 else 0\n",
    "        # Element-level (sum of orbitals)\n",
    "        raw[el]['w_VBM'] = sum(raw[el][f'{o}_VBM_norm'] for o in ['s','p','d'])\n",
    "        raw[el]['w_CBM'] = sum(raw[el][f'{o}_CBM_norm'] for o in ['s','p','d'])\n",
    "        # p-orbital fraction for this element\n",
    "        raw[el]['p_VBM'] = raw[el]['p_VBM_norm']\n",
    "        raw[el]['p_CBM'] = raw[el]['p_CBM_norm']\n",
    "    \n",
    "    return raw\n",
    "\n",
    "def compute_descriptors(raw):\n",
    "    \"\"\"\n",
    "    From raw contributions dict, compute all descriptor types.\n",
    "    Returns flat dict of descriptors.\n",
    "    \"\"\"\n",
    "    desc = {}\n",
    "    elements = list(raw.keys())\n",
    "    \n",
    "    # Sort by mass (heaviest first)\n",
    "    sorted_els = sorted(elements, key=lambda x: raw[x]['mass'], reverse=True)\n",
    "    heavy1 = sorted_els[0] if len(sorted_els) >= 1 else None\n",
    "    heavy2 = sorted_els[1] if len(sorted_els) >= 2 else None\n",
    "    \n",
    "    # --- Type A: WM_total = sum(w_X * M_X) ---\n",
    "    desc['A_WM_VBM'] = sum(raw[el]['w_VBM'] * raw[el]['mass'] for el in elements)\n",
    "    desc['A_WM_CBM'] = sum(raw[el]['w_CBM'] * raw[el]['mass'] for el in elements)\n",
    "    \n",
    "    # --- Type B: WM_p_only = sum(p_X * M_X) ---\n",
    "    desc['B_WMp_VBM'] = sum(raw[el]['p_VBM'] * raw[el]['mass'] for el in elements)\n",
    "    desc['B_WMp_CBM'] = sum(raw[el]['p_CBM'] * raw[el]['mass'] for el in elements)\n",
    "    \n",
    "    # --- Type C: WM_p_frac = p_i*M_i / sum(p_j*M_j) for top 2 heaviest ---\n",
    "    denom_vbm = sum(raw[el]['p_VBM'] * raw[el]['mass'] for el in elements)\n",
    "    denom_cbm = sum(raw[el]['p_CBM'] * raw[el]['mass'] for el in elements)\n",
    "    \n",
    "    if heavy1:\n",
    "        desc['C_pfrac_h1_VBM'] = (raw[heavy1]['p_VBM'] * raw[heavy1]['mass']) / denom_vbm if denom_vbm > 0 else 0\n",
    "        desc['C_pfrac_h1_CBM'] = (raw[heavy1]['p_CBM'] * raw[heavy1]['mass']) / denom_cbm if denom_cbm > 0 else 0\n",
    "    else:\n",
    "        desc['C_pfrac_h1_VBM'] = 0\n",
    "        desc['C_pfrac_h1_CBM'] = 0\n",
    "    \n",
    "    if heavy2:\n",
    "        desc['C_pfrac_h2_VBM'] = (raw[heavy2]['p_VBM'] * raw[heavy2]['mass']) / denom_vbm if denom_vbm > 0 else 0\n",
    "        desc['C_pfrac_h2_CBM'] = (raw[heavy2]['p_CBM'] * raw[heavy2]['mass']) / denom_cbm if denom_cbm > 0 else 0\n",
    "    else:\n",
    "        desc['C_pfrac_h2_VBM'] = 0\n",
    "        desc['C_pfrac_h2_CBM'] = 0\n",
    "    \n",
    "    # --- Type D: WM_indiv = w_heavyN * M_heavyN ---\n",
    "    if heavy1:\n",
    "        desc['D_wm_h1_VBM'] = raw[heavy1]['w_VBM'] * raw[heavy1]['mass']\n",
    "        desc['D_wm_h1_CBM'] = raw[heavy1]['w_CBM'] * raw[heavy1]['mass']\n",
    "    else:\n",
    "        desc['D_wm_h1_VBM'] = 0\n",
    "        desc['D_wm_h1_CBM'] = 0\n",
    "    \n",
    "    if heavy2:\n",
    "        desc['D_wm_h2_VBM'] = raw[heavy2]['w_VBM'] * raw[heavy2]['mass']\n",
    "        desc['D_wm_h2_CBM'] = raw[heavy2]['w_CBM'] * raw[heavy2]['mass']\n",
    "    else:\n",
    "        desc['D_wm_h2_VBM'] = 0\n",
    "        desc['D_wm_h2_CBM'] = 0\n",
    "    \n",
    "    # --- Type E: p_frac = total p-orbital % ---\n",
    "    desc['E_pfrac_VBM'] = sum(raw[el]['p_VBM'] for el in elements)\n",
    "    desc['E_pfrac_CBM'] = sum(raw[el]['p_CBM'] for el in elements)\n",
    "    \n",
    "    # --- Type F: p_heavy = p_frac_of_heaviest * M_heaviest ---\n",
    "    if heavy1:\n",
    "        desc['F_ph1_VBM'] = raw[heavy1]['p_VBM'] * raw[heavy1]['mass']\n",
    "        desc['F_ph1_CBM'] = raw[heavy1]['p_CBM'] * raw[heavy1]['mass']\n",
    "    else:\n",
    "        desc['F_ph1_VBM'] = 0\n",
    "        desc['F_ph1_CBM'] = 0\n",
    "    \n",
    "    if heavy2:\n",
    "        desc['F_ph2_VBM'] = raw[heavy2]['p_VBM'] * raw[heavy2]['mass']\n",
    "        desc['F_ph2_CBM'] = raw[heavy2]['p_CBM'] * raw[heavy2]['mass']\n",
    "    else:\n",
    "        desc['F_ph2_VBM'] = 0\n",
    "        desc['F_ph2_CBM'] = 0\n",
    "    \n",
    "    # --- Metadata ---\n",
    "    desc['heavy1_el'] = heavy1 if heavy1 else 'NA'\n",
    "    desc['heavy2_el'] = heavy2 if heavy2 else 'NA'\n",
    "    desc['heavy1_mass'] = raw[heavy1]['mass'] if heavy1 else 0\n",
    "    desc['heavy2_mass'] = raw[heavy2]['mass'] if heavy2 else 0\n",
    "    desc['n_elements'] = len(elements)\n",
    "    \n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processed: 0 compounds\n",
      "Failed: 99\n",
      "Failed compounds:\n",
      "  SSeW (001e03f2c095): no_folder\n",
      "  Sn2Te2 (03bcf7dcdaf2): no_folder\n",
      "  ClSbTe (04fdd7d1ec5c): no_folder\n",
      "  WMo3Se8 (05a06afa3b20): no_folder\n",
      "  CrW3Se8 (0b7696e1f4c9): no_folder\n",
      "  ClSbSe (0c0fbdaf8f4a): no_folder\n",
      "  ISbTe (0f02957b17cf): no_folder\n",
      "  AsITe (114b3382699c): no_folder\n",
      "  BiBrSe (11db0908d9ef): no_folder\n",
      "  CrMo3Te8 (159f028a85d0): no_folder\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BATCH PROCESS ALL COMPOUNDS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {w: [] for w in WINDOWS}\n",
    "failed = []\n",
    "\n",
    "for idx, row in target.iterrows():\n",
    "    uid = row['uid']\n",
    "    formula = row['Formula']\n",
    "    alpha_R = row['alpha_R_max']\n",
    "    \n",
    "    if uid not in compound_dirs:\n",
    "        failed.append({'uid': uid, 'formula': formula, 'reason': 'no_folder'})\n",
    "        continue\n",
    "    \n",
    "    vasprun_path = compound_dirs[uid]['vasprun']\n",
    "    \n",
    "    print(f\"[{idx+1}/{len(target)}] {formula} ({uid[:12]}...)\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        for w in WINDOWS:\n",
    "            raw = extract_contributions(vasprun_path, w)\n",
    "            desc = compute_descriptors(raw)\n",
    "            desc['uid'] = uid\n",
    "            desc['Formula'] = formula\n",
    "            desc['alpha_R'] = alpha_R\n",
    "            desc['window'] = w\n",
    "            all_results[w].append(desc)\n",
    "        print(\"OK\")\n",
    "    except Exception as e:\n",
    "        failed.append({'uid': uid, 'formula': formula, 'reason': str(e)[:80]})\n",
    "        print(f\"FAILED: {str(e)[:60]}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Processed: {len(all_results[WINDOWS[0]])} compounds\")\n",
    "print(f\"Failed: {len(failed)}\")\n",
    "if failed:\n",
    "    print(\"Failed compounds:\")\n",
    "    for f in failed[:10]:\n",
    "        print(f\"  {f['formula']} ({f['uid'][:12]}): {f['reason']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['uid', 'Formula', 'alpha_R', 'heavy1_el', 'heavy2_el', 'heavy1_mass',\\n       'heavy2_mass', 'n_elements', 'A_WM_VBM', 'A_WM_CBM'],\\n      dtype='str')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m     csv_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdesc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_w\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m     csv_path = os.path.join(OUTPUT_DIR, csv_name)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m.to_csv(csv_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     28\u001b[39m     saved_csvs.append(csv_name)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Combined: all types for this window\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AbCMS_Lab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4384\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4383\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4384\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4386\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AbCMS_Lab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6302\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6300\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6302\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6304\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6306\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AbCMS_Lab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6352\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6352\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6354\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['uid', 'Formula', 'alpha_R', 'heavy1_el', 'heavy2_el', 'heavy1_mass',\\n       'heavy2_mass', 'n_elements', 'A_WM_VBM', 'A_WM_CBM'],\\n      dtype='str')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BUILD DATAFRAMES & SAVE CSVs\n",
    "# ============================================================\n",
    "\n",
    "# Descriptor type -> column prefixes\n",
    "type_cols = {\n",
    "    'A': [c for c in ['A_WM_VBM', 'A_WM_CBM']],\n",
    "    'B': [c for c in ['B_WMp_VBM', 'B_WMp_CBM']],\n",
    "    'C': [c for c in ['C_pfrac_h1_VBM', 'C_pfrac_h1_CBM', 'C_pfrac_h2_VBM', 'C_pfrac_h2_CBM']],\n",
    "    'D': [c for c in ['D_wm_h1_VBM', 'D_wm_h1_CBM', 'D_wm_h2_VBM', 'D_wm_h2_CBM']],\n",
    "    'E': [c for c in ['E_pfrac_VBM', 'E_pfrac_CBM']],\n",
    "    'F': [c for c in ['F_ph1_VBM', 'F_ph1_CBM', 'F_ph2_VBM', 'F_ph2_CBM']],\n",
    "}\n",
    "meta_cols = ['uid', 'Formula', 'alpha_R', 'heavy1_el', 'heavy2_el', 'heavy1_mass', 'heavy2_mass', 'n_elements']\n",
    "\n",
    "saved_csvs = []\n",
    "\n",
    "for w in WINDOWS:\n",
    "    df = pd.DataFrame(all_results[w])\n",
    "    w_str = str(w).replace('.', '')\n",
    "    \n",
    "    # Individual type CSVs\n",
    "    for t_name, t_cols in type_cols.items():\n",
    "        cols = meta_cols + t_cols\n",
    "        csv_name = f\"desc_{t_name}_w{w_str}.csv\"\n",
    "        csv_path = os.path.join(OUTPUT_DIR, csv_name)\n",
    "        df[cols].to_csv(csv_path, index=False)\n",
    "        saved_csvs.append(csv_name)\n",
    "    \n",
    "    # Combined: all types for this window\n",
    "    all_feature_cols = meta_cols + [c for cols in type_cols.values() for c in cols]\n",
    "    csv_name = f\"desc_ALL_w{w_str}.csv\"\n",
    "    csv_path = os.path.join(OUTPUT_DIR, csv_name)\n",
    "    df[all_feature_cols].to_csv(csv_path, index=False)\n",
    "    saved_csvs.append(csv_name)\n",
    "\n",
    "# Also: some useful combos across windows\n",
    "# Combo 1: Type A across all windows\n",
    "combo_rows = []\n",
    "for w in WINDOWS:\n",
    "    df_w = pd.DataFrame(all_results[w])\n",
    "    w_str = str(w).replace('.', '')\n",
    "    rename = {c: f\"{c}_w{w_str}\" for c in ['A_WM_VBM','A_WM_CBM','B_WMp_VBM','B_WMp_CBM',\n",
    "              'E_pfrac_VBM','E_pfrac_CBM','D_wm_h1_VBM','D_wm_h1_CBM',\n",
    "              'F_ph1_VBM','F_ph1_CBM']}\n",
    "    for old, new in rename.items():\n",
    "        if old in df_w.columns:\n",
    "            df_w[new] = df_w[old]\n",
    "\n",
    "# Multi-window combined\n",
    "dfs_w = {}\n",
    "for w in WINDOWS:\n",
    "    w_str = str(w).replace('.', '')\n",
    "    df_w = pd.DataFrame(all_results[w])\n",
    "    feature_cols_w = [c for cols in type_cols.values() for c in cols]\n",
    "    df_w = df_w[['uid', 'Formula', 'alpha_R'] + feature_cols_w]\n",
    "    df_w = df_w.rename(columns={c: f\"{c}_w{w_str}\" for c in feature_cols_w})\n",
    "    dfs_w[w] = df_w\n",
    "\n",
    "# Merge all windows\n",
    "df_multi = dfs_w[WINDOWS[0]]\n",
    "for w in WINDOWS[1:]:\n",
    "    df_multi = df_multi.merge(dfs_w[w].drop(columns=['Formula', 'alpha_R']), on='uid')\n",
    "\n",
    "csv_name = \"desc_ALL_multiwindow.csv\"\n",
    "csv_path = os.path.join(OUTPUT_DIR, csv_name)\n",
    "df_multi.to_csv(csv_path, index=False)\n",
    "saved_csvs.append(csv_name)\n",
    "\n",
    "print(f\"\\nSaved {len(saved_csvs)} CSV files to {OUTPUT_DIR}:\")\n",
    "for name in sorted(saved_csvs):\n",
    "    fpath = os.path.join(OUTPUT_DIR, name)\n",
    "    size = os.path.getsize(fpath) if os.path.exists(fpath) else 0\n",
    "    print(f\"  {name} ({size/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK SANITY CHECK\n",
    "# ============================================================\n",
    "df_check = pd.read_csv(os.path.join(OUTPUT_DIR, f\"desc_ALL_w005.csv\"))\n",
    "print(f\"Shape: {df_check.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df_check.head())\n",
    "print(f\"\\nDescriptor stats:\")\n",
    "feature_cols = [c for c in df_check.columns if c.startswith(('A_','B_','C_','D_','E_','F_'))]\n",
    "print(df_check[feature_cols].describe().round(3))\n",
    "print(f\"\\nalpha_R range: [{df_check['alpha_R'].min():.3f}, {df_check['alpha_R'].max():.3f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
